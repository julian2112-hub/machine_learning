{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6x3Hn9MIb9k"
      },
      "source": [
        "# Assignment <span style=\"color:red\">option Four</span> - News Categorization using PyTorch\n",
        "\n",
        "Download the dataset from https://www.kaggle.com/uciml/news-aggregator-dataset and develop a news classification or categorization model. The dataset contain only titles of a news item and some metadata. The categories of the news items include one of: –<span  style=\"color:red\"> b</span> : business – <span  style=\"color:red\">t</span> : science and technology – <span  style=\"color:red\">e</span> : entertainment and –<span  style=\"color:red\">m</span> : health.\n",
        "\n",
        "1. Prepare training and test dataset: Split the data into training and test set (80% train and 20% test). Make sure they are balanced, otherwise if all <span  style=\"color:red\">b</span> files are on training, your model fails to predict <span  style=\"color:red\">t</span> files in test.\n",
        "2. Binary classification: produce training data for each two categories, such as <span  style=\"color:red\">b </span> and <span  style=\"color:red\"> t</span>, <span  style=\"color:red\">b</span> and <span  style=\"color:red\"> m</span>, <span  style=\"color:red\">e</span> and <span  style=\"color:red\">t</span> and so on. Evaluate the performance and report which categories are easier for the models.\n",
        "3. Adapt the Text Categorization PyTorch code (see above) and evaluate the performance of the system for these task\n",
        "4. Use a pre-trained embeddings and compare your result. When you use pre-trained embeddings, you have to average the word embeddings of each tokens in ach document to get the unique representation of the document. DOC_EMBEDDING = (TOKEN1_EMBEDDING + ... + TOKENn_EMBEDDING). You can also use some of the <span  style=\"color:red\">spacy/FLAIR </span>document embedding methods\n",
        "5. Report the recall, precision, and F1 scores for both binary and multi-class classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwyNWM5nr-jL"
      },
      "source": [
        "# Task 1\n",
        "\n",
        "1. Prepare training and test dataset: Split the data into training and test set (80% train and 20% test). Make sure they are balanced, otherwise if all <span  style=\"color:red\">b</span> files are on training, your model fails to predict <span  style=\"color:red\">t</span> files in test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RnOhGjir-jM",
        "outputId": "c7a099b9-39de-4432-9494-f6f43df4d83b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainingsdaten:  337935\n",
            "Testdaten:  84484\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# read data\n",
        "data = pd.read_csv(\"data/uci-news-aggregator.csv\")\n",
        "# remove unnecessary columns\n",
        "frame = data[[\"TITLE\", \"CATEGORY\"]]\n",
        "\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "# Division into training and test data. The stratify parameter causes the \"Category\" feature to be split equally\n",
        "training_data, testing_data = train_test_split(\n",
        "    frame, test_size=TEST_SIZE, random_state=0, stratify=data[\"CATEGORY\"]\n",
        ")\n",
        "\n",
        "# print size of train and test set\n",
        "print(\"Trainingsdaten: \", len(training_data))\n",
        "print(\"Testdaten: \", len(testing_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLqPPvzgr-jO"
      },
      "source": [
        "# Task 2\n",
        "\n",
        "Binary classification: produce training data for each two categories, such as b and t, b\n",
        "and m, e and t and so on. Evaluate the performance and report which categories are\n",
        "easier for the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WI4nJ3Wr-jP",
        "outputId": "2046c38a-bb94-4589-f504-5c024f375b7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_12476\\1080574781.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_12476\\1080574781.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Category Pair: b (1) vs t (0)\n",
            "------------------PERFORMANCE-----------------------------\n",
            "Accuracy: 0.93\n",
            "Precision: 0.93\n",
            "Recall: 0.93\n",
            "F1_score: 0.93\n",
            "--------------------REPORT--------------------------------\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.92     21669\n",
            "           1       0.93      0.93      0.93     23193\n",
            "\n",
            "    accuracy                           0.93     44862\n",
            "   macro avg       0.93      0.93      0.93     44862\n",
            "weighted avg       0.93      0.93      0.93     44862\n",
            "\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_12476\\1080574781.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_12476\\1080574781.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Category Pair: b (1) vs e (0)\n",
            "------------------PERFORMANCE-----------------------------\n",
            "Accuracy: 0.98\n",
            "Precision: 0.98\n",
            "Recall: 0.97\n",
            "F1_score: 0.97\n",
            "--------------------REPORT--------------------------------\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98     30494\n",
            "           1       0.98      0.97      0.97     23193\n",
            "\n",
            "    accuracy                           0.98     53687\n",
            "   macro avg       0.98      0.98      0.98     53687\n",
            "weighted avg       0.98      0.98      0.98     53687\n",
            "\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_12476\\1080574781.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_12476\\1080574781.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Category Pair: b (1) vs m (0)\n",
            "------------------PERFORMANCE-----------------------------\n",
            "Accuracy: 0.97\n",
            "Precision: 0.97\n",
            "Recall: 0.99\n",
            "F1_score: 0.98\n",
            "--------------------REPORT--------------------------------\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95      9128\n",
            "           1       0.97      0.99      0.98     23193\n",
            "\n",
            "    accuracy                           0.97     32321\n",
            "   macro avg       0.97      0.96      0.97     32321\n",
            "weighted avg       0.97      0.97      0.97     32321\n",
            "\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_12476\\1080574781.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_12476\\1080574781.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Category Pair: t (1) vs e (0)\n",
            "------------------PERFORMANCE-----------------------------\n",
            "Accuracy: 0.98\n",
            "Precision: 0.97\n",
            "Recall: 0.97\n",
            "F1_score: 0.97\n",
            "--------------------REPORT--------------------------------\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98     30494\n",
            "           1       0.97      0.97      0.97     21669\n",
            "\n",
            "    accuracy                           0.98     52163\n",
            "   macro avg       0.98      0.98      0.98     52163\n",
            "weighted avg       0.98      0.98      0.98     52163\n",
            "\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_12476\\1080574781.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_12476\\1080574781.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Category Pair: t (1) vs m (0)\n",
            "------------------PERFORMANCE-----------------------------\n",
            "Accuracy: 0.98\n",
            "Precision: 0.97\n",
            "Recall: 0.99\n",
            "F1_score: 0.98\n",
            "--------------------REPORT--------------------------------\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96      9128\n",
            "           1       0.97      0.99      0.98     21669\n",
            "\n",
            "    accuracy                           0.98     30797\n",
            "   macro avg       0.98      0.97      0.97     30797\n",
            "weighted avg       0.98      0.98      0.98     30797\n",
            "\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_12476\\1080574781.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_12476\\1080574781.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Category Pair: e (1) vs m (0)\n",
            "------------------PERFORMANCE-----------------------------\n",
            "Accuracy: 0.98\n",
            "Precision: 0.98\n",
            "Recall: 0.99\n",
            "F1_score: 0.99\n",
            "--------------------REPORT--------------------------------\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.95      9128\n",
            "           1       0.98      0.99      0.99     30494\n",
            "\n",
            "    accuracy                           0.98     39622\n",
            "   macro avg       0.98      0.96      0.97     39622\n",
            "weighted avg       0.98      0.98      0.98     39622\n",
            "\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Define the categories\n",
        "categories = [\"b\", \"t\", \"e\", \"m\"]\n",
        "\n",
        "# get all possible combinations\n",
        "combinations_categories = list(combinations(categories, 2))\n",
        "\n",
        "# print combinations\n",
        "# for combination in possible_combinations:\n",
        "#    print(combination)\n",
        "\n",
        "# loop through each category combination\n",
        "for category_pair in combinations_categories:\n",
        "    category_1, category_2 = category_pair\n",
        "\n",
        "    # only keep data of category pair\n",
        "    filtered_training_data = training_data[\n",
        "        (training_data[\"CATEGORY\"] == category_1)\n",
        "        | (training_data[\"CATEGORY\"] == category_2)\n",
        "    ]\n",
        "    filtered_test_data = testing_data[\n",
        "        (testing_data[\"CATEGORY\"] == category_1)\n",
        "        | (testing_data[\"CATEGORY\"] == category_2)\n",
        "    ]\n",
        "\n",
        "    # Create a binary dataset for the current category pair\n",
        "    cat_mapping = {category_1: 1, category_2: 0}\n",
        "    filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
        "        \"CATEGORY\"\n",
        "    ].map(cat_mapping)\n",
        "    filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n",
        "        cat_mapping\n",
        "    )\n",
        "\n",
        "    # print(filtered_training_data)\n",
        "\n",
        "    # split the binary dataset into features (X) und labels (y)\n",
        "    X_train = filtered_training_data[\"TITLE\"]\n",
        "    y_train = filtered_training_data[\"CATEGORY_IN_BINARY\"]\n",
        "    X_test = filtered_test_data[\"TITLE\"]\n",
        "    y_test = filtered_test_data[\"CATEGORY_IN_BINARY\"]\n",
        "\n",
        "    # vectorize the titles using TF-IDF\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "    # save vectorizer\n",
        "    with open(f'vectorizer/tasktwo_{category_1}_{category_2}.pkl', 'wb') as vectorizer_file:\n",
        "        pickle.dump(vectorizer, vectorizer_file)\n",
        "\n",
        "    # train a Naive Bayes classifier\n",
        "    classifier = MultinomialNB()\n",
        "    classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "\n",
        "\n",
        "    # Save the trained model\n",
        "    with open(f'models/tasktwo_{category_1}_{category_2}.pkl', 'wb') as model_file:\n",
        "        pickle.dump(classifier, model_file)\n",
        "\n",
        "    # make predictions on the test set\n",
        "    predictions = classifier.predict(X_test_tfidf)\n",
        "\n",
        "    # evaluate performance\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions)\n",
        "    recall = recall_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions)\n",
        "\n",
        "    # this report gives further information\n",
        "    report = classification_report(y_test, predictions)\n",
        "\n",
        "    # print results\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    print(\n",
        "        f\"Category Pair: {category_1} ({cat_mapping[category_1]}) vs {category_2} ({cat_mapping[category_2]})\"\n",
        "    )\n",
        "    print(\"------------------PERFORMANCE-----------------------------\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1_score: {f1:.2f}\")\n",
        "    print(\"--------------------REPORT--------------------------------\")\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT_Ao7jPr-jQ"
      },
      "source": [
        "# Task 3\n",
        "\n",
        "Adapt the Text Categorization PyTorch code (see above) and evaluate the performance\n",
        "of the system for these task\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg_zbCysYO_B",
        "outputId": "ed5c1a4b-f6b2-4034-c28d-79fbef226d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "135402\n",
            "--------------------\n",
            "Verwendetes Gerät: cpu\n",
            "Epoch [1/1], Step [4/2252], Loss: 1.2026\n",
            "Epoch [1/1], Step [8/2252], Loss: 0.9040\n",
            "Epoch [1/1], Step [12/2252], Loss: 0.7454\n",
            "Epoch [1/1], Step [16/2252], Loss: 0.8232\n",
            "Epoch [1/1], Step [20/2252], Loss: 0.7347\n",
            "Epoch [1/1], Step [24/2252], Loss: 0.5539\n",
            "Epoch [1/1], Step [28/2252], Loss: 0.4534\n",
            "Epoch [1/1], Step [32/2252], Loss: 0.6852\n",
            "Epoch [1/1], Step [36/2252], Loss: 0.4110\n",
            "Epoch [1/1], Step [40/2252], Loss: 0.5999\n",
            "Epoch [1/1], Step [44/2252], Loss: 0.5751\n",
            "Epoch [1/1], Step [48/2252], Loss: 0.5467\n",
            "Epoch [1/1], Step [52/2252], Loss: 0.4947\n",
            "Epoch [1/1], Step [56/2252], Loss: 0.4191\n",
            "Epoch [1/1], Step [60/2252], Loss: 0.4290\n",
            "Epoch [1/1], Step [64/2252], Loss: 0.4390\n",
            "Epoch [1/1], Step [68/2252], Loss: 0.4488\n",
            "Epoch [1/1], Step [72/2252], Loss: 0.4311\n",
            "Epoch [1/1], Step [76/2252], Loss: 0.3283\n",
            "Epoch [1/1], Step [80/2252], Loss: 0.5231\n",
            "Epoch [1/1], Step [84/2252], Loss: 0.4287\n",
            "Epoch [1/1], Step [88/2252], Loss: 0.5416\n",
            "Epoch [1/1], Step [92/2252], Loss: 0.4271\n",
            "Epoch [1/1], Step [96/2252], Loss: 0.3718\n",
            "Epoch [1/1], Step [100/2252], Loss: 0.3191\n",
            "Epoch [1/1], Step [104/2252], Loss: 0.3961\n",
            "Epoch [1/1], Step [108/2252], Loss: 0.5216\n",
            "Epoch [1/1], Step [112/2252], Loss: 0.4295\n",
            "Epoch [1/1], Step [116/2252], Loss: 0.3685\n",
            "Epoch [1/1], Step [120/2252], Loss: 0.4227\n",
            "Epoch [1/1], Step [124/2252], Loss: 0.2519\n",
            "Epoch [1/1], Step [128/2252], Loss: 0.3192\n",
            "Epoch [1/1], Step [132/2252], Loss: 0.4438\n",
            "Epoch [1/1], Step [136/2252], Loss: 0.4802\n",
            "Epoch [1/1], Step [140/2252], Loss: 0.4193\n",
            "Epoch [1/1], Step [144/2252], Loss: 0.3842\n",
            "Epoch [1/1], Step [148/2252], Loss: 0.3818\n",
            "Epoch [1/1], Step [152/2252], Loss: 0.2669\n",
            "Epoch [1/1], Step [156/2252], Loss: 0.2681\n",
            "Epoch [1/1], Step [160/2252], Loss: 0.4285\n",
            "Epoch [1/1], Step [164/2252], Loss: 0.3437\n",
            "Epoch [1/1], Step [168/2252], Loss: 0.3169\n",
            "Epoch [1/1], Step [172/2252], Loss: 0.3008\n",
            "Epoch [1/1], Step [176/2252], Loss: 0.2997\n",
            "Epoch [1/1], Step [180/2252], Loss: 0.3250\n",
            "Epoch [1/1], Step [184/2252], Loss: 0.2598\n",
            "Epoch [1/1], Step [188/2252], Loss: 0.2715\n",
            "Epoch [1/1], Step [192/2252], Loss: 0.2909\n",
            "Epoch [1/1], Step [196/2252], Loss: 0.2726\n",
            "Epoch [1/1], Step [200/2252], Loss: 0.2740\n",
            "Epoch [1/1], Step [204/2252], Loss: 0.3566\n",
            "Epoch [1/1], Step [208/2252], Loss: 0.3093\n",
            "Epoch [1/1], Step [212/2252], Loss: 0.2990\n",
            "Epoch [1/1], Step [216/2252], Loss: 0.2739\n",
            "Epoch [1/1], Step [220/2252], Loss: 0.2200\n",
            "Epoch [1/1], Step [224/2252], Loss: 0.4171\n",
            "Epoch [1/1], Step [228/2252], Loss: 0.3936\n",
            "Epoch [1/1], Step [232/2252], Loss: 0.2372\n",
            "Epoch [1/1], Step [236/2252], Loss: 0.3419\n",
            "Epoch [1/1], Step [240/2252], Loss: 0.2508\n",
            "Epoch [1/1], Step [244/2252], Loss: 0.3831\n",
            "Epoch [1/1], Step [248/2252], Loss: 0.3028\n",
            "Epoch [1/1], Step [252/2252], Loss: 0.4099\n",
            "Epoch [1/1], Step [256/2252], Loss: 0.3708\n",
            "Epoch [1/1], Step [260/2252], Loss: 0.3573\n",
            "Epoch [1/1], Step [264/2252], Loss: 0.2801\n",
            "Epoch [1/1], Step [268/2252], Loss: 0.2851\n",
            "Epoch [1/1], Step [272/2252], Loss: 0.2195\n",
            "Epoch [1/1], Step [276/2252], Loss: 0.1985\n",
            "Epoch [1/1], Step [280/2252], Loss: 0.1711\n",
            "Epoch [1/1], Step [284/2252], Loss: 0.1724\n",
            "Epoch [1/1], Step [288/2252], Loss: 0.3617\n",
            "Epoch [1/1], Step [292/2252], Loss: 0.2583\n",
            "Epoch [1/1], Step [296/2252], Loss: 0.2685\n",
            "Epoch [1/1], Step [300/2252], Loss: 0.2225\n",
            "Epoch [1/1], Step [304/2252], Loss: 0.3195\n",
            "Epoch [1/1], Step [308/2252], Loss: 0.2898\n",
            "Epoch [1/1], Step [312/2252], Loss: 0.3612\n",
            "Epoch [1/1], Step [316/2252], Loss: 0.3182\n",
            "Epoch [1/1], Step [320/2252], Loss: 0.2694\n",
            "Epoch [1/1], Step [324/2252], Loss: 0.3165\n",
            "Epoch [1/1], Step [328/2252], Loss: 0.2347\n",
            "Epoch [1/1], Step [332/2252], Loss: 0.1676\n",
            "Epoch [1/1], Step [336/2252], Loss: 0.2013\n",
            "Epoch [1/1], Step [340/2252], Loss: 0.3092\n",
            "Epoch [1/1], Step [344/2252], Loss: 0.3146\n",
            "Epoch [1/1], Step [348/2252], Loss: 0.3538\n",
            "Epoch [1/1], Step [352/2252], Loss: 0.3551\n",
            "Epoch [1/1], Step [356/2252], Loss: 0.3244\n",
            "Epoch [1/1], Step [360/2252], Loss: 0.2796\n",
            "Epoch [1/1], Step [364/2252], Loss: 0.2619\n",
            "Epoch [1/1], Step [368/2252], Loss: 0.3174\n",
            "Epoch [1/1], Step [372/2252], Loss: 0.3049\n",
            "Epoch [1/1], Step [376/2252], Loss: 0.2888\n",
            "Epoch [1/1], Step [380/2252], Loss: 0.3391\n",
            "Epoch [1/1], Step [384/2252], Loss: 0.2791\n",
            "Epoch [1/1], Step [388/2252], Loss: 0.3312\n",
            "Epoch [1/1], Step [392/2252], Loss: 0.4996\n",
            "Epoch [1/1], Step [396/2252], Loss: 0.2273\n",
            "Epoch [1/1], Step [400/2252], Loss: 0.2903\n",
            "Epoch [1/1], Step [404/2252], Loss: 0.3930\n",
            "Epoch [1/1], Step [408/2252], Loss: 0.1904\n",
            "Epoch [1/1], Step [412/2252], Loss: 0.2958\n",
            "Epoch [1/1], Step [416/2252], Loss: 0.3732\n",
            "Epoch [1/1], Step [420/2252], Loss: 0.3423\n",
            "Epoch [1/1], Step [424/2252], Loss: 0.3188\n",
            "Epoch [1/1], Step [428/2252], Loss: 0.2644\n",
            "Epoch [1/1], Step [432/2252], Loss: 0.2828\n",
            "Epoch [1/1], Step [436/2252], Loss: 0.2964\n",
            "Epoch [1/1], Step [440/2252], Loss: 0.3787\n",
            "Epoch [1/1], Step [444/2252], Loss: 0.1375\n",
            "Epoch [1/1], Step [448/2252], Loss: 0.2956\n",
            "Epoch [1/1], Step [452/2252], Loss: 0.2046\n",
            "Epoch [1/1], Step [456/2252], Loss: 0.2219\n",
            "Epoch [1/1], Step [460/2252], Loss: 0.2313\n",
            "Epoch [1/1], Step [464/2252], Loss: 0.2644\n",
            "Epoch [1/1], Step [468/2252], Loss: 0.1500\n",
            "Epoch [1/1], Step [472/2252], Loss: 0.3203\n",
            "Epoch [1/1], Step [476/2252], Loss: 0.3454\n",
            "Epoch [1/1], Step [480/2252], Loss: 0.2229\n",
            "Epoch [1/1], Step [484/2252], Loss: 0.2426\n",
            "Epoch [1/1], Step [488/2252], Loss: 0.2781\n",
            "Epoch [1/1], Step [492/2252], Loss: 0.2966\n",
            "Epoch [1/1], Step [496/2252], Loss: 0.2590\n",
            "Epoch [1/1], Step [500/2252], Loss: 0.2395\n",
            "Epoch [1/1], Step [504/2252], Loss: 0.2937\n",
            "Epoch [1/1], Step [508/2252], Loss: 0.2890\n",
            "Epoch [1/1], Step [512/2252], Loss: 0.3301\n",
            "Epoch [1/1], Step [516/2252], Loss: 0.3864\n",
            "Epoch [1/1], Step [520/2252], Loss: 0.2781\n",
            "Epoch [1/1], Step [524/2252], Loss: 0.2604\n",
            "Epoch [1/1], Step [528/2252], Loss: 0.3564\n",
            "Epoch [1/1], Step [532/2252], Loss: 0.2195\n",
            "Epoch [1/1], Step [536/2252], Loss: 0.2527\n",
            "Epoch [1/1], Step [540/2252], Loss: 0.1855\n",
            "Epoch [1/1], Step [544/2252], Loss: 0.1871\n",
            "Epoch [1/1], Step [548/2252], Loss: 0.3835\n",
            "Epoch [1/1], Step [552/2252], Loss: 0.2120\n",
            "Epoch [1/1], Step [556/2252], Loss: 0.5278\n",
            "Epoch [1/1], Step [560/2252], Loss: 0.2787\n",
            "Epoch [1/1], Step [564/2252], Loss: 0.3083\n",
            "Epoch [1/1], Step [568/2252], Loss: 0.2452\n",
            "Epoch [1/1], Step [572/2252], Loss: 0.4229\n",
            "Epoch [1/1], Step [576/2252], Loss: 0.3445\n",
            "Epoch [1/1], Step [580/2252], Loss: 0.2523\n",
            "Epoch [1/1], Step [584/2252], Loss: 0.2068\n",
            "Epoch [1/1], Step [588/2252], Loss: 0.1992\n",
            "Epoch [1/1], Step [592/2252], Loss: 0.1991\n",
            "Epoch [1/1], Step [596/2252], Loss: 0.3597\n",
            "Epoch [1/1], Step [600/2252], Loss: 0.2554\n",
            "Epoch [1/1], Step [604/2252], Loss: 0.2878\n",
            "Epoch [1/1], Step [608/2252], Loss: 0.2274\n",
            "Epoch [1/1], Step [612/2252], Loss: 0.2857\n",
            "Epoch [1/1], Step [616/2252], Loss: 0.1397\n",
            "Epoch [1/1], Step [620/2252], Loss: 0.3405\n",
            "Epoch [1/1], Step [624/2252], Loss: 0.2161\n",
            "Epoch [1/1], Step [628/2252], Loss: 0.1874\n",
            "Epoch [1/1], Step [632/2252], Loss: 0.1780\n",
            "Epoch [1/1], Step [636/2252], Loss: 0.1758\n",
            "Epoch [1/1], Step [640/2252], Loss: 0.2482\n",
            "Epoch [1/1], Step [644/2252], Loss: 0.2791\n",
            "Epoch [1/1], Step [648/2252], Loss: 0.4026\n",
            "Epoch [1/1], Step [652/2252], Loss: 0.2172\n",
            "Epoch [1/1], Step [656/2252], Loss: 0.1525\n",
            "Epoch [1/1], Step [660/2252], Loss: 0.2402\n",
            "Epoch [1/1], Step [664/2252], Loss: 0.3212\n",
            "Epoch [1/1], Step [668/2252], Loss: 0.2977\n",
            "Epoch [1/1], Step [672/2252], Loss: 0.2433\n",
            "Epoch [1/1], Step [676/2252], Loss: 0.2347\n",
            "Epoch [1/1], Step [680/2252], Loss: 0.3106\n",
            "Epoch [1/1], Step [684/2252], Loss: 0.3253\n",
            "Epoch [1/1], Step [688/2252], Loss: 0.4585\n",
            "Epoch [1/1], Step [692/2252], Loss: 0.1922\n",
            "Epoch [1/1], Step [696/2252], Loss: 0.2293\n",
            "Epoch [1/1], Step [700/2252], Loss: 0.1766\n",
            "Epoch [1/1], Step [704/2252], Loss: 0.3315\n",
            "Epoch [1/1], Step [708/2252], Loss: 0.2525\n",
            "Epoch [1/1], Step [712/2252], Loss: 0.2139\n",
            "Epoch [1/1], Step [716/2252], Loss: 0.3483\n",
            "Epoch [1/1], Step [720/2252], Loss: 0.2664\n",
            "Epoch [1/1], Step [724/2252], Loss: 0.2368\n",
            "Epoch [1/1], Step [728/2252], Loss: 0.2362\n",
            "Epoch [1/1], Step [732/2252], Loss: 0.1641\n",
            "Epoch [1/1], Step [736/2252], Loss: 0.2267\n",
            "Epoch [1/1], Step [740/2252], Loss: 0.3698\n",
            "Epoch [1/1], Step [744/2252], Loss: 0.1902\n",
            "Epoch [1/1], Step [748/2252], Loss: 0.1703\n",
            "Epoch [1/1], Step [752/2252], Loss: 0.2406\n",
            "Epoch [1/1], Step [756/2252], Loss: 0.3897\n",
            "Epoch [1/1], Step [760/2252], Loss: 0.1663\n",
            "Epoch [1/1], Step [764/2252], Loss: 0.1906\n",
            "Epoch [1/1], Step [768/2252], Loss: 0.3097\n",
            "Epoch [1/1], Step [772/2252], Loss: 0.1071\n",
            "Epoch [1/1], Step [776/2252], Loss: 0.2375\n",
            "Epoch [1/1], Step [780/2252], Loss: 0.2089\n",
            "Epoch [1/1], Step [784/2252], Loss: 0.2304\n",
            "Epoch [1/1], Step [788/2252], Loss: 0.2598\n",
            "Epoch [1/1], Step [792/2252], Loss: 0.1808\n",
            "Epoch [1/1], Step [796/2252], Loss: 0.2264\n",
            "Epoch [1/1], Step [800/2252], Loss: 0.3174\n",
            "Epoch [1/1], Step [804/2252], Loss: 0.2313\n",
            "Epoch [1/1], Step [808/2252], Loss: 0.3794\n",
            "Epoch [1/1], Step [812/2252], Loss: 0.3079\n",
            "Epoch [1/1], Step [816/2252], Loss: 0.1361\n",
            "Epoch [1/1], Step [820/2252], Loss: 0.3803\n",
            "Epoch [1/1], Step [824/2252], Loss: 0.2801\n",
            "Epoch [1/1], Step [828/2252], Loss: 0.2584\n",
            "Epoch [1/1], Step [832/2252], Loss: 0.2929\n",
            "Epoch [1/1], Step [836/2252], Loss: 0.2837\n",
            "Epoch [1/1], Step [840/2252], Loss: 0.1340\n",
            "Epoch [1/1], Step [844/2252], Loss: 0.1613\n",
            "Epoch [1/1], Step [848/2252], Loss: 0.2832\n",
            "Epoch [1/1], Step [852/2252], Loss: 0.2791\n",
            "Epoch [1/1], Step [856/2252], Loss: 0.3637\n",
            "Epoch [1/1], Step [860/2252], Loss: 0.2843\n",
            "Epoch [1/1], Step [864/2252], Loss: 0.2193\n",
            "Epoch [1/1], Step [868/2252], Loss: 0.3010\n",
            "Epoch [1/1], Step [872/2252], Loss: 0.2954\n",
            "Epoch [1/1], Step [876/2252], Loss: 0.3120\n",
            "Epoch [1/1], Step [880/2252], Loss: 0.2085\n",
            "Epoch [1/1], Step [884/2252], Loss: 0.1720\n",
            "Epoch [1/1], Step [888/2252], Loss: 0.2704\n",
            "Epoch [1/1], Step [892/2252], Loss: 0.2033\n",
            "Epoch [1/1], Step [896/2252], Loss: 0.2087\n",
            "Epoch [1/1], Step [900/2252], Loss: 0.2333\n",
            "Epoch [1/1], Step [904/2252], Loss: 0.2963\n",
            "Epoch [1/1], Step [908/2252], Loss: 0.5150\n",
            "Epoch [1/1], Step [912/2252], Loss: 0.2684\n",
            "Epoch [1/1], Step [916/2252], Loss: 0.1352\n",
            "Epoch [1/1], Step [920/2252], Loss: 0.1037\n",
            "Epoch [1/1], Step [924/2252], Loss: 0.5741\n",
            "Epoch [1/1], Step [928/2252], Loss: 0.2705\n",
            "Epoch [1/1], Step [932/2252], Loss: 0.3559\n",
            "Epoch [1/1], Step [936/2252], Loss: 0.1336\n",
            "Epoch [1/1], Step [940/2252], Loss: 0.1855\n",
            "Epoch [1/1], Step [944/2252], Loss: 0.1134\n",
            "Epoch [1/1], Step [948/2252], Loss: 0.3514\n",
            "Epoch [1/1], Step [952/2252], Loss: 0.3404\n",
            "Epoch [1/1], Step [956/2252], Loss: 0.2395\n",
            "Epoch [1/1], Step [960/2252], Loss: 0.2367\n",
            "Epoch [1/1], Step [964/2252], Loss: 0.1943\n",
            "Epoch [1/1], Step [968/2252], Loss: 0.4463\n",
            "Epoch [1/1], Step [972/2252], Loss: 0.2848\n",
            "Epoch [1/1], Step [976/2252], Loss: 0.2463\n",
            "Epoch [1/1], Step [980/2252], Loss: 0.2636\n",
            "Epoch [1/1], Step [984/2252], Loss: 0.2680\n",
            "Epoch [1/1], Step [988/2252], Loss: 0.3331\n",
            "Epoch [1/1], Step [992/2252], Loss: 0.2319\n",
            "Epoch [1/1], Step [996/2252], Loss: 0.1545\n",
            "Epoch [1/1], Step [1000/2252], Loss: 0.4111\n",
            "Epoch [1/1], Step [1004/2252], Loss: 0.2845\n",
            "Epoch [1/1], Step [1008/2252], Loss: 0.2185\n",
            "Epoch [1/1], Step [1012/2252], Loss: 0.3104\n",
            "Epoch [1/1], Step [1016/2252], Loss: 0.3373\n",
            "Epoch [1/1], Step [1020/2252], Loss: 0.1209\n",
            "Epoch [1/1], Step [1024/2252], Loss: 0.2232\n",
            "Epoch [1/1], Step [1028/2252], Loss: 0.2439\n",
            "Epoch [1/1], Step [1032/2252], Loss: 0.2284\n",
            "Epoch [1/1], Step [1036/2252], Loss: 0.1595\n",
            "Epoch [1/1], Step [1040/2252], Loss: 0.2728\n",
            "Epoch [1/1], Step [1044/2252], Loss: 0.3839\n",
            "Epoch [1/1], Step [1048/2252], Loss: 0.1374\n",
            "Epoch [1/1], Step [1052/2252], Loss: 0.2295\n",
            "Epoch [1/1], Step [1056/2252], Loss: 0.1946\n",
            "Epoch [1/1], Step [1060/2252], Loss: 0.1556\n",
            "Epoch [1/1], Step [1064/2252], Loss: 0.2650\n",
            "Epoch [1/1], Step [1068/2252], Loss: 0.2413\n",
            "Epoch [1/1], Step [1072/2252], Loss: 0.2487\n",
            "Epoch [1/1], Step [1076/2252], Loss: 0.2502\n",
            "Epoch [1/1], Step [1080/2252], Loss: 0.2235\n",
            "Epoch [1/1], Step [1084/2252], Loss: 0.1776\n",
            "Epoch [1/1], Step [1088/2252], Loss: 0.0804\n",
            "Epoch [1/1], Step [1092/2252], Loss: 0.2897\n",
            "Epoch [1/1], Step [1096/2252], Loss: 0.3832\n",
            "Epoch [1/1], Step [1100/2252], Loss: 0.2014\n",
            "Epoch [1/1], Step [1104/2252], Loss: 0.2026\n",
            "Epoch [1/1], Step [1108/2252], Loss: 0.1433\n",
            "Epoch [1/1], Step [1112/2252], Loss: 0.2097\n",
            "Epoch [1/1], Step [1116/2252], Loss: 0.3232\n",
            "Epoch [1/1], Step [1120/2252], Loss: 0.2385\n",
            "Epoch [1/1], Step [1124/2252], Loss: 0.3614\n",
            "Epoch [1/1], Step [1128/2252], Loss: 0.2098\n",
            "Epoch [1/1], Step [1132/2252], Loss: 0.2037\n",
            "Epoch [1/1], Step [1136/2252], Loss: 0.3303\n",
            "Epoch [1/1], Step [1140/2252], Loss: 0.2809\n",
            "Epoch [1/1], Step [1144/2252], Loss: 0.1603\n",
            "Epoch [1/1], Step [1148/2252], Loss: 0.2518\n",
            "Epoch [1/1], Step [1152/2252], Loss: 0.3642\n",
            "Epoch [1/1], Step [1156/2252], Loss: 0.2249\n",
            "Epoch [1/1], Step [1160/2252], Loss: 0.1952\n",
            "Epoch [1/1], Step [1164/2252], Loss: 0.3885\n",
            "Epoch [1/1], Step [1168/2252], Loss: 0.4659\n",
            "Epoch [1/1], Step [1172/2252], Loss: 0.1878\n",
            "Epoch [1/1], Step [1176/2252], Loss: 0.1819\n",
            "Epoch [1/1], Step [1180/2252], Loss: 0.2538\n",
            "Epoch [1/1], Step [1184/2252], Loss: 0.2802\n",
            "Epoch [1/1], Step [1188/2252], Loss: 0.2311\n",
            "Epoch [1/1], Step [1192/2252], Loss: 0.1955\n",
            "Epoch [1/1], Step [1196/2252], Loss: 0.1358\n",
            "Epoch [1/1], Step [1200/2252], Loss: 0.3180\n",
            "Epoch [1/1], Step [1204/2252], Loss: 0.2400\n",
            "Epoch [1/1], Step [1208/2252], Loss: 0.1712\n",
            "Epoch [1/1], Step [1212/2252], Loss: 0.1454\n",
            "Epoch [1/1], Step [1216/2252], Loss: 0.1340\n",
            "Epoch [1/1], Step [1220/2252], Loss: 0.1598\n",
            "Epoch [1/1], Step [1224/2252], Loss: 0.2671\n",
            "Epoch [1/1], Step [1228/2252], Loss: 0.6036\n",
            "Epoch [1/1], Step [1232/2252], Loss: 0.2555\n",
            "Epoch [1/1], Step [1236/2252], Loss: 0.3878\n",
            "Epoch [1/1], Step [1240/2252], Loss: 0.2006\n",
            "Epoch [1/1], Step [1244/2252], Loss: 0.2775\n",
            "Epoch [1/1], Step [1248/2252], Loss: 0.4155\n",
            "Epoch [1/1], Step [1252/2252], Loss: 0.2664\n",
            "Epoch [1/1], Step [1256/2252], Loss: 0.2884\n",
            "Epoch [1/1], Step [1260/2252], Loss: 0.3223\n",
            "Epoch [1/1], Step [1264/2252], Loss: 0.3206\n",
            "Epoch [1/1], Step [1268/2252], Loss: 0.1584\n",
            "Epoch [1/1], Step [1272/2252], Loss: 0.2826\n",
            "Epoch [1/1], Step [1276/2252], Loss: 0.1798\n",
            "Epoch [1/1], Step [1280/2252], Loss: 0.1101\n",
            "Epoch [1/1], Step [1284/2252], Loss: 0.1582\n",
            "Epoch [1/1], Step [1288/2252], Loss: 0.1633\n",
            "Epoch [1/1], Step [1292/2252], Loss: 0.1596\n",
            "Epoch [1/1], Step [1296/2252], Loss: 0.3011\n",
            "Epoch [1/1], Step [1300/2252], Loss: 0.2882\n",
            "Epoch [1/1], Step [1304/2252], Loss: 0.1211\n",
            "Epoch [1/1], Step [1308/2252], Loss: 0.2840\n",
            "Epoch [1/1], Step [1312/2252], Loss: 0.2816\n",
            "Epoch [1/1], Step [1316/2252], Loss: 0.1208\n",
            "Epoch [1/1], Step [1320/2252], Loss: 0.2174\n",
            "Epoch [1/1], Step [1324/2252], Loss: 0.3339\n",
            "Epoch [1/1], Step [1328/2252], Loss: 0.3769\n",
            "Epoch [1/1], Step [1332/2252], Loss: 0.3620\n",
            "Epoch [1/1], Step [1336/2252], Loss: 0.1988\n",
            "Epoch [1/1], Step [1340/2252], Loss: 0.2619\n",
            "Epoch [1/1], Step [1344/2252], Loss: 0.2795\n",
            "Epoch [1/1], Step [1348/2252], Loss: 0.1906\n",
            "Epoch [1/1], Step [1352/2252], Loss: 0.2302\n",
            "Epoch [1/1], Step [1356/2252], Loss: 0.2009\n",
            "Epoch [1/1], Step [1360/2252], Loss: 0.2701\n",
            "Epoch [1/1], Step [1364/2252], Loss: 0.2195\n",
            "Epoch [1/1], Step [1368/2252], Loss: 0.2284\n",
            "Epoch [1/1], Step [1372/2252], Loss: 0.1432\n",
            "Epoch [1/1], Step [1376/2252], Loss: 0.1237\n",
            "Epoch [1/1], Step [1380/2252], Loss: 0.2338\n",
            "Epoch [1/1], Step [1384/2252], Loss: 0.3268\n",
            "Epoch [1/1], Step [1388/2252], Loss: 0.1251\n",
            "Epoch [1/1], Step [1392/2252], Loss: 0.1427\n",
            "Epoch [1/1], Step [1396/2252], Loss: 0.1637\n",
            "Epoch [1/1], Step [1400/2252], Loss: 0.1211\n",
            "Epoch [1/1], Step [1404/2252], Loss: 0.3501\n",
            "Epoch [1/1], Step [1408/2252], Loss: 0.2153\n",
            "Epoch [1/1], Step [1412/2252], Loss: 0.2852\n",
            "Epoch [1/1], Step [1416/2252], Loss: 0.2501\n",
            "Epoch [1/1], Step [1420/2252], Loss: 0.1995\n",
            "Epoch [1/1], Step [1424/2252], Loss: 0.1951\n",
            "Epoch [1/1], Step [1428/2252], Loss: 0.2744\n",
            "Epoch [1/1], Step [1432/2252], Loss: 0.2353\n",
            "Epoch [1/1], Step [1436/2252], Loss: 0.1969\n",
            "Epoch [1/1], Step [1440/2252], Loss: 0.2917\n",
            "Epoch [1/1], Step [1444/2252], Loss: 0.1390\n",
            "Epoch [1/1], Step [1448/2252], Loss: 0.1998\n",
            "Epoch [1/1], Step [1452/2252], Loss: 0.2467\n",
            "Epoch [1/1], Step [1456/2252], Loss: 0.2698\n",
            "Epoch [1/1], Step [1460/2252], Loss: 0.3576\n",
            "Epoch [1/1], Step [1464/2252], Loss: 0.1443\n",
            "Epoch [1/1], Step [1468/2252], Loss: 0.1973\n",
            "Epoch [1/1], Step [1472/2252], Loss: 0.3102\n",
            "Epoch [1/1], Step [1476/2252], Loss: 0.2185\n",
            "Epoch [1/1], Step [1480/2252], Loss: 0.3460\n",
            "Epoch [1/1], Step [1484/2252], Loss: 1.3638\n",
            "Epoch [1/1], Step [1488/2252], Loss: 0.2755\n",
            "Epoch [1/1], Step [1492/2252], Loss: 0.2627\n",
            "Epoch [1/1], Step [1496/2252], Loss: 0.2131\n",
            "Epoch [1/1], Step [1500/2252], Loss: 0.2162\n",
            "Epoch [1/1], Step [1504/2252], Loss: 0.2261\n",
            "Epoch [1/1], Step [1508/2252], Loss: 0.3730\n",
            "Epoch [1/1], Step [1512/2252], Loss: 0.2130\n",
            "Epoch [1/1], Step [1516/2252], Loss: 0.3493\n",
            "Epoch [1/1], Step [1520/2252], Loss: 0.2279\n",
            "Epoch [1/1], Step [1524/2252], Loss: 0.2305\n",
            "Epoch [1/1], Step [1528/2252], Loss: 0.2169\n",
            "Epoch [1/1], Step [1532/2252], Loss: 0.2984\n",
            "Epoch [1/1], Step [1536/2252], Loss: 0.3114\n",
            "Epoch [1/1], Step [1540/2252], Loss: 0.2303\n",
            "Epoch [1/1], Step [1544/2252], Loss: 0.2551\n",
            "Epoch [1/1], Step [1548/2252], Loss: 0.2915\n",
            "Epoch [1/1], Step [1552/2252], Loss: 0.1986\n",
            "Epoch [1/1], Step [1556/2252], Loss: 0.2273\n",
            "Epoch [1/1], Step [1560/2252], Loss: 0.2103\n",
            "Epoch [1/1], Step [1564/2252], Loss: 0.2059\n",
            "Epoch [1/1], Step [1568/2252], Loss: 0.3147\n",
            "Epoch [1/1], Step [1572/2252], Loss: 0.1576\n",
            "Epoch [1/1], Step [1576/2252], Loss: 0.1791\n",
            "Epoch [1/1], Step [1580/2252], Loss: 0.4201\n",
            "Epoch [1/1], Step [1584/2252], Loss: 0.5665\n",
            "Epoch [1/1], Step [1588/2252], Loss: 0.1408\n",
            "Epoch [1/1], Step [1592/2252], Loss: 0.3092\n",
            "Epoch [1/1], Step [1596/2252], Loss: 0.2819\n",
            "Epoch [1/1], Step [1600/2252], Loss: 0.1496\n",
            "Epoch [1/1], Step [1604/2252], Loss: 0.1848\n",
            "Epoch [1/1], Step [1608/2252], Loss: 0.1827\n",
            "Epoch [1/1], Step [1612/2252], Loss: 0.3982\n",
            "Epoch [1/1], Step [1616/2252], Loss: 0.2221\n",
            "Epoch [1/1], Step [1620/2252], Loss: 0.2339\n",
            "Epoch [1/1], Step [1624/2252], Loss: 0.1052\n",
            "Epoch [1/1], Step [1628/2252], Loss: 0.1636\n",
            "Epoch [1/1], Step [1632/2252], Loss: 0.2131\n",
            "Epoch [1/1], Step [1636/2252], Loss: 0.3421\n",
            "Epoch [1/1], Step [1640/2252], Loss: 0.2551\n",
            "Epoch [1/1], Step [1644/2252], Loss: 0.2256\n",
            "Epoch [1/1], Step [1648/2252], Loss: 0.3788\n",
            "Epoch [1/1], Step [1652/2252], Loss: 0.1697\n",
            "Epoch [1/1], Step [1656/2252], Loss: 0.1905\n",
            "Epoch [1/1], Step [1660/2252], Loss: 0.2325\n",
            "Epoch [1/1], Step [1664/2252], Loss: 0.2420\n",
            "Epoch [1/1], Step [1668/2252], Loss: 0.1237\n",
            "Epoch [1/1], Step [1672/2252], Loss: 0.3439\n",
            "Epoch [1/1], Step [1676/2252], Loss: 0.3090\n",
            "Epoch [1/1], Step [1680/2252], Loss: 0.1447\n",
            "Epoch [1/1], Step [1684/2252], Loss: 0.1227\n",
            "Epoch [1/1], Step [1688/2252], Loss: 0.3600\n",
            "Epoch [1/1], Step [1692/2252], Loss: 0.2289\n",
            "Epoch [1/1], Step [1696/2252], Loss: 0.3312\n",
            "Epoch [1/1], Step [1700/2252], Loss: 0.2365\n",
            "Epoch [1/1], Step [1704/2252], Loss: 0.1897\n",
            "Epoch [1/1], Step [1708/2252], Loss: 0.2771\n",
            "Epoch [1/1], Step [1712/2252], Loss: 0.2426\n",
            "Epoch [1/1], Step [1716/2252], Loss: 0.2136\n",
            "Epoch [1/1], Step [1720/2252], Loss: 0.2522\n",
            "Epoch [1/1], Step [1724/2252], Loss: 0.1770\n",
            "Epoch [1/1], Step [1728/2252], Loss: 0.2896\n",
            "Epoch [1/1], Step [1732/2252], Loss: 0.3083\n",
            "Epoch [1/1], Step [1736/2252], Loss: 0.2076\n",
            "Epoch [1/1], Step [1740/2252], Loss: 0.1621\n",
            "Epoch [1/1], Step [1744/2252], Loss: 0.2299\n",
            "Epoch [1/1], Step [1748/2252], Loss: 0.2158\n",
            "Epoch [1/1], Step [1752/2252], Loss: 0.3015\n",
            "Epoch [1/1], Step [1756/2252], Loss: 0.2750\n",
            "Epoch [1/1], Step [1760/2252], Loss: 0.2431\n",
            "Epoch [1/1], Step [1764/2252], Loss: 0.3935\n",
            "Epoch [1/1], Step [1768/2252], Loss: 0.1953\n",
            "Epoch [1/1], Step [1772/2252], Loss: 0.1883\n",
            "Epoch [1/1], Step [1776/2252], Loss: 0.1505\n",
            "Epoch [1/1], Step [1780/2252], Loss: 0.2027\n",
            "Epoch [1/1], Step [1784/2252], Loss: 0.3257\n",
            "Epoch [1/1], Step [1788/2252], Loss: 0.3678\n",
            "Epoch [1/1], Step [1792/2252], Loss: 0.1654\n",
            "Epoch [1/1], Step [1796/2252], Loss: 0.3103\n",
            "Epoch [1/1], Step [1800/2252], Loss: 0.2261\n",
            "Epoch [1/1], Step [1804/2252], Loss: 0.1674\n",
            "Epoch [1/1], Step [1808/2252], Loss: 0.2112\n",
            "Epoch [1/1], Step [1812/2252], Loss: 0.2154\n",
            "Epoch [1/1], Step [1816/2252], Loss: 0.1189\n",
            "Epoch [1/1], Step [1820/2252], Loss: 0.2110\n",
            "Epoch [1/1], Step [1824/2252], Loss: 0.2762\n",
            "Epoch [1/1], Step [1828/2252], Loss: 0.2939\n",
            "Epoch [1/1], Step [1832/2252], Loss: 0.4098\n",
            "Epoch [1/1], Step [1836/2252], Loss: 0.1606\n",
            "Epoch [1/1], Step [1840/2252], Loss: 0.2647\n",
            "Epoch [1/1], Step [1844/2252], Loss: 0.1674\n",
            "Epoch [1/1], Step [1848/2252], Loss: 0.2769\n",
            "Epoch [1/1], Step [1852/2252], Loss: 0.2494\n",
            "Epoch [1/1], Step [1856/2252], Loss: 0.2465\n",
            "Epoch [1/1], Step [1860/2252], Loss: 0.2458\n",
            "Epoch [1/1], Step [1864/2252], Loss: 0.2647\n",
            "Epoch [1/1], Step [1868/2252], Loss: 0.3299\n",
            "Epoch [1/1], Step [1872/2252], Loss: 0.2929\n",
            "Epoch [1/1], Step [1876/2252], Loss: 0.2418\n",
            "Epoch [1/1], Step [1880/2252], Loss: 0.2186\n",
            "Epoch [1/1], Step [1884/2252], Loss: 0.2335\n",
            "Epoch [1/1], Step [1888/2252], Loss: 0.1727\n",
            "Epoch [1/1], Step [1892/2252], Loss: 0.3085\n",
            "Epoch [1/1], Step [1896/2252], Loss: 0.2041\n",
            "Epoch [1/1], Step [1900/2252], Loss: 0.1334\n",
            "Epoch [1/1], Step [1904/2252], Loss: 0.1788\n",
            "Epoch [1/1], Step [1908/2252], Loss: 0.2178\n",
            "Epoch [1/1], Step [1912/2252], Loss: 0.2969\n",
            "Epoch [1/1], Step [1916/2252], Loss: 0.1876\n",
            "Epoch [1/1], Step [1920/2252], Loss: 0.2886\n",
            "Epoch [1/1], Step [1924/2252], Loss: 0.1920\n",
            "Epoch [1/1], Step [1928/2252], Loss: 0.1862\n",
            "Epoch [1/1], Step [1932/2252], Loss: 0.2250\n",
            "Epoch [1/1], Step [1936/2252], Loss: 0.1806\n",
            "Epoch [1/1], Step [1940/2252], Loss: 0.3104\n",
            "Epoch [1/1], Step [1944/2252], Loss: 0.3746\n",
            "Epoch [1/1], Step [1948/2252], Loss: 0.1962\n",
            "Epoch [1/1], Step [1952/2252], Loss: 0.3180\n",
            "Epoch [1/1], Step [1956/2252], Loss: 0.3236\n",
            "Epoch [1/1], Step [1960/2252], Loss: 0.1790\n",
            "Epoch [1/1], Step [1964/2252], Loss: 0.2112\n",
            "Epoch [1/1], Step [1968/2252], Loss: 0.1079\n",
            "Epoch [1/1], Step [1972/2252], Loss: 0.2560\n",
            "Epoch [1/1], Step [1976/2252], Loss: 0.2065\n",
            "Epoch [1/1], Step [1980/2252], Loss: 0.1815\n",
            "Epoch [1/1], Step [1984/2252], Loss: 0.2548\n",
            "Epoch [1/1], Step [1988/2252], Loss: 0.2019\n",
            "Epoch [1/1], Step [1992/2252], Loss: 0.2252\n",
            "Epoch [1/1], Step [1996/2252], Loss: 0.2787\n",
            "Epoch [1/1], Step [2000/2252], Loss: 0.2430\n",
            "Epoch [1/1], Step [2004/2252], Loss: 0.1976\n",
            "Epoch [1/1], Step [2008/2252], Loss: 0.2165\n",
            "Epoch [1/1], Step [2012/2252], Loss: 0.1950\n",
            "Epoch [1/1], Step [2016/2252], Loss: 0.2327\n",
            "Epoch [1/1], Step [2020/2252], Loss: 0.0858\n",
            "Epoch [1/1], Step [2024/2252], Loss: 0.3042\n",
            "Epoch [1/1], Step [2028/2252], Loss: 0.2095\n",
            "Epoch [1/1], Step [2032/2252], Loss: 0.2105\n",
            "Epoch [1/1], Step [2036/2252], Loss: 0.3410\n",
            "Epoch [1/1], Step [2040/2252], Loss: 0.2384\n",
            "Epoch [1/1], Step [2044/2252], Loss: 0.3345\n",
            "Epoch [1/1], Step [2048/2252], Loss: 0.3662\n",
            "Epoch [1/1], Step [2052/2252], Loss: 0.1884\n",
            "Epoch [1/1], Step [2056/2252], Loss: 0.2247\n",
            "Epoch [1/1], Step [2060/2252], Loss: 0.1600\n",
            "Epoch [1/1], Step [2064/2252], Loss: 0.2994\n",
            "Epoch [1/1], Step [2068/2252], Loss: 0.1545\n",
            "Epoch [1/1], Step [2072/2252], Loss: 0.2783\n",
            "Epoch [1/1], Step [2076/2252], Loss: 0.2232\n",
            "Epoch [1/1], Step [2080/2252], Loss: 0.3799\n",
            "Epoch [1/1], Step [2084/2252], Loss: 0.2183\n",
            "Epoch [1/1], Step [2088/2252], Loss: 0.2261\n",
            "Epoch [1/1], Step [2092/2252], Loss: 0.1323\n",
            "Epoch [1/1], Step [2096/2252], Loss: 0.1435\n",
            "Epoch [1/1], Step [2100/2252], Loss: 0.2775\n",
            "Epoch [1/1], Step [2104/2252], Loss: 0.2324\n",
            "Epoch [1/1], Step [2108/2252], Loss: 0.2251\n",
            "Epoch [1/1], Step [2112/2252], Loss: 0.1875\n",
            "Epoch [1/1], Step [2116/2252], Loss: 0.1849\n",
            "Epoch [1/1], Step [2120/2252], Loss: 0.3001\n",
            "Epoch [1/1], Step [2124/2252], Loss: 0.1642\n",
            "Epoch [1/1], Step [2128/2252], Loss: 0.1905\n",
            "Epoch [1/1], Step [2132/2252], Loss: 0.1245\n",
            "Epoch [1/1], Step [2136/2252], Loss: 0.1567\n",
            "Epoch [1/1], Step [2140/2252], Loss: 0.2115\n",
            "Epoch [1/1], Step [2144/2252], Loss: 0.3673\n",
            "Epoch [1/1], Step [2148/2252], Loss: 0.2004\n",
            "Epoch [1/1], Step [2152/2252], Loss: 0.2423\n",
            "Epoch [1/1], Step [2156/2252], Loss: 0.1645\n",
            "Epoch [1/1], Step [2160/2252], Loss: 0.2783\n",
            "Epoch [1/1], Step [2164/2252], Loss: 0.1800\n",
            "Epoch [1/1], Step [2168/2252], Loss: 0.2517\n",
            "Epoch [1/1], Step [2172/2252], Loss: 0.2441\n",
            "Epoch [1/1], Step [2176/2252], Loss: 0.1039\n",
            "Epoch [1/1], Step [2180/2252], Loss: 0.1381\n",
            "Epoch [1/1], Step [2184/2252], Loss: 0.3687\n",
            "Epoch [1/1], Step [2188/2252], Loss: 0.2633\n",
            "Epoch [1/1], Step [2192/2252], Loss: 0.1426\n",
            "Epoch [1/1], Step [2196/2252], Loss: 0.2477\n",
            "Epoch [1/1], Step [2200/2252], Loss: 0.2795\n",
            "Epoch [1/1], Step [2204/2252], Loss: 0.4080\n",
            "Epoch [1/1], Step [2208/2252], Loss: 0.2387\n",
            "Epoch [1/1], Step [2212/2252], Loss: 0.2664\n",
            "Epoch [1/1], Step [2216/2252], Loss: 0.2435\n",
            "Epoch [1/1], Step [2220/2252], Loss: 0.2080\n",
            "Epoch [1/1], Step [2224/2252], Loss: 0.2982\n",
            "Epoch [1/1], Step [2228/2252], Loss: 0.2619\n",
            "Epoch [1/1], Step [2232/2252], Loss: 0.2208\n",
            "Epoch [1/1], Step [2236/2252], Loss: 0.1614\n",
            "Epoch [1/1], Step [2240/2252], Loss: 0.1860\n",
            "Epoch [1/1], Step [2244/2252], Loss: 0.2991\n",
            "Epoch [1/1], Step [2248/2252], Loss: 0.1588\n",
            "Epoch [1/1], Step [2252/2252], Loss: 0.1484\n",
            "Name---> layer_1.weight \n",
            "Values---> tensor([[-2.9803e-01, -4.5892e-01, -2.0395e+00,  ..., -1.0588e-03,\n",
            "         -2.6283e-03, -2.5493e-03],\n",
            "        [ 2.3572e+00, -1.5580e-01, -1.7572e+00,  ..., -1.4002e-03,\n",
            "         -2.6482e-03, -1.4742e-03],\n",
            "        [-1.5154e+00, -1.0405e+00, -1.0414e+00,  ...,  8.6059e-04,\n",
            "          2.6740e-03, -2.1552e-03],\n",
            "        ...,\n",
            "        [ 1.3360e+00,  2.7076e+00, -2.3731e+00,  ...,  2.6791e-03,\n",
            "          1.6358e-03, -2.4365e-03],\n",
            "        [ 9.8364e-01,  7.6494e-01, -2.7565e+00,  ...,  9.2202e-04,\n",
            "         -8.4572e-04, -1.2031e-03],\n",
            "        [ 2.2405e+00,  1.5069e+00, -2.3100e+00,  ...,  1.0672e-03,\n",
            "          1.0055e-03, -2.3389e-03]])\n",
            "Name---> layer_1.bias \n",
            "Values---> tensor([-2.4830, -1.7222, -2.4019, -0.8190, -3.7309, -2.4133, -1.1797,  1.4115,\n",
            "         0.2078, -6.1108, -1.5952, -3.4020, -3.3589, -1.9211, -5.5633,  0.3533,\n",
            "        -3.4562, -3.7113, -0.8362, -3.0430, -4.8374, -4.3722, -4.5959, -1.7732,\n",
            "        -2.5974, -0.5711, -2.6869, -4.2391, -6.5781, -2.4422, -1.0207,  0.9636,\n",
            "         0.0435, -1.7125, -2.3645, -4.5157, -2.4872, -5.8911, -2.7374, -4.9574,\n",
            "        -2.2379, -5.3478, -4.5597,  0.0901, -2.5463, -3.2327, -8.4419,  0.7599,\n",
            "        -4.1609,  0.7182, -3.3439, -4.9222, -1.0547, -1.5303, -8.9070, -3.2606,\n",
            "        -0.6375, -1.0164, -1.2993, -3.2716, -3.0622, -5.3093, -3.3113,  1.2592,\n",
            "        -4.2706, -4.8540, -2.9231, -4.1993, -0.0503, -2.8469, -0.3827, -5.9597,\n",
            "        -3.5213, -5.9142, -3.6548, -3.2277, -3.6992, -2.3197, -2.8753, -1.8330,\n",
            "        -5.7963, -7.5119, -1.0211, -5.0821, -4.5630, -0.5567, -1.2828, -6.4901,\n",
            "        -1.4039, -2.0846, -2.7641,  2.7684, -3.6343, -3.6445, -1.5885, -3.0204,\n",
            "        -4.6030, -2.8070, -5.8950, -2.5112])\n",
            "Name---> layer_2.weight \n",
            "Values---> tensor([[-0.2377, -0.2382, -0.2990,  ..., -0.3314, -0.2262, -0.3428],\n",
            "        [-1.9266, -1.6326,  0.0457,  ..., -0.3011, -1.4852, -0.2297],\n",
            "        [-0.2602, -0.3099,  0.6847,  ..., -0.3017, -1.6077, -0.2111],\n",
            "        ...,\n",
            "        [-0.8233, -0.0262, -0.4519,  ..., -0.1803,  0.7715,  0.3140],\n",
            "        [-0.2938, -1.3095, -1.6001,  ..., -0.9227, -1.5510, -1.6510],\n",
            "        [-0.2632, -0.2700, -0.2679,  ..., -0.3358, -0.2828, -0.2818]])\n",
            "Name---> layer_2.bias \n",
            "Values---> tensor([-0.3003, -3.5270, -1.8204, -1.3416, -0.2142, -0.2193, -0.5309, -0.3073,\n",
            "        -2.1033, -0.7231, -0.6375,  0.0082, -0.0880, -0.2941,  2.6652, -0.5452,\n",
            "        -1.8229, -0.1171, -0.0436, -2.3310, -0.2753, -3.9405, -0.0921, -0.3884,\n",
            "         0.0083, -0.3039, -0.0894, -0.3199, -0.5595, -0.5463, -0.4406, -1.9023,\n",
            "        -1.3643, -0.0332, -0.2850, -0.2791, -0.0719, -0.2341, -0.5069, -0.6508,\n",
            "        -0.3718, -2.5228, -0.7508, -3.0580, -3.3447, -0.8966, -0.2936, -0.2207,\n",
            "        -0.4111, -0.8344, -2.2444, -2.3241, -0.2876, -3.9668, -1.5984, -1.8472,\n",
            "        -0.4615, -3.1775, -0.6554, -0.8216, -0.3655, -0.2268, -0.7105, -1.9514,\n",
            "        -0.8590, -1.4451, -1.4960, -1.1035, -1.6935,  1.3986, -0.0871, -0.3925,\n",
            "         0.1741, -0.2447, -1.6068, -0.3291, -2.0045, -0.9517, -1.1239, -0.7800,\n",
            "        -0.2245, -0.1693, -0.2794, -1.8831, -0.4205, -1.6368, -0.7715,  3.1149,\n",
            "        -0.3036, -0.5698, -0.5765, -0.5383, -1.0860, -0.5718, -0.3631, -0.3769,\n",
            "        -0.2269,  4.3708, -2.0528, -0.2190])\n",
            "Name---> output_layer.weight \n",
            "Values---> tensor([[-2.9984e-01,  2.5367e-01,  6.6334e-01,  6.6816e-01,  2.5516e-01,\n",
            "          2.8409e-01, -3.7911e-01, -1.9852e-01, -8.1619e-01,  1.7092e-01,\n",
            "          1.1569e-01, -6.1784e-02,  2.6208e-01, -3.5384e-02, -1.7857e-02,\n",
            "         -3.0085e-01, -1.2241e+00,  4.4715e-01, -6.9071e-02,  1.9290e-01,\n",
            "          1.3959e-01,  1.7534e-01, -1.5443e-01,  5.2492e-01,  5.6621e-01,\n",
            "          7.1735e-02,  2.5236e-01, -2.6534e-01, -1.7685e-01, -7.9313e-01,\n",
            "         -2.3857e-01, -8.0657e-01,  3.5369e-01,  7.5309e-02, -2.5500e-01,\n",
            "         -1.4995e-01, -6.4902e-02,  3.7033e-01,  2.1441e-01,  1.6547e-01,\n",
            "         -2.7753e-01, -1.0926e-01, -6.0065e-01, -7.5809e-01, -2.2561e-01,\n",
            "          1.5427e-01,  4.7135e-02,  2.9241e-01, -3.8556e-01,  1.1609e-01,\n",
            "         -1.1672e+00,  1.5759e+00,  2.1976e-01,  4.3441e-01, -1.5081e+00,\n",
            "         -4.4468e-01, -1.6129e-01,  8.3935e-01, -3.1177e-01,  1.0696e-01,\n",
            "         -3.5835e-01,  2.0334e-01,  3.7190e-01, -3.6628e-01,  2.9398e-01,\n",
            "         -4.1389e-05,  2.7068e-01,  3.8427e-01,  2.3076e-01,  1.0219e-01,\n",
            "         -1.3337e-01, -3.0473e-01,  5.6420e-02,  2.1473e-01,  1.0835e-01,\n",
            "          6.6615e-02,  1.1094e-01,  1.2569e+00, -1.1336e-01,  2.5111e-04,\n",
            "         -7.4821e-01, -1.2551e-01,  2.8763e-01,  9.0366e-01, -4.6818e-01,\n",
            "          2.8860e-01, -5.8706e-01,  2.4225e-01, -1.9601e-01,  9.1690e-01,\n",
            "         -1.2317e-01,  1.6288e-01, -4.2256e-01, -1.0892e-02, -3.1772e-01,\n",
            "          1.6301e-01,  2.6206e-01, -1.1005e-01,  9.0956e-01,  2.7463e-01],\n",
            "        [ 2.3610e-01, -2.9052e-01, -3.4383e-01, -1.0851e+00, -2.2961e-01,\n",
            "         -3.4583e-01,  4.1420e-01,  2.4911e-01, -1.4782e+00, -4.9418e-01,\n",
            "          2.2448e-01, -5.6800e-02,  1.8126e-01, -1.1092e-02, -1.3181e-01,\n",
            "         -6.7866e-01,  1.3296e+00, -2.8153e-01, -4.1068e-02, -1.5618e+00,\n",
            "          1.6309e-01, -6.7744e-01,  1.1966e-01, -5.3567e-03, -4.4893e-01,\n",
            "          1.2196e-01, -2.1777e-01, -2.5424e-01,  5.3247e-01,  2.7070e-01,\n",
            "         -1.5808e-01,  9.7565e-01,  1.4838e-01,  2.9697e-03, -2.5750e-01,\n",
            "         -2.7480e-01,  3.5095e-05, -2.0895e-01, -5.1047e-01,  1.2106e+00,\n",
            "          1.9335e-01, -7.7722e-01, -2.5153e-02,  5.8723e-01,  4.7780e-01,\n",
            "          1.1616e-01,  1.6446e-01, -2.0717e-01, -1.5548e-01, -2.6028e-01,\n",
            "          1.2210e+00, -5.3778e-01, -3.2359e-01,  1.1848e+00,  6.7130e-01,\n",
            "          4.9822e-01, -2.2660e-01, -6.7618e-01,  2.4749e-01,  7.6814e-02,\n",
            "          4.5669e-01, -4.1005e-01, -3.3792e-01, -2.9152e-01,  5.1548e-01,\n",
            "         -2.4555e-01,  8.0396e-02, -4.2562e-01,  4.2379e-01,  2.6092e-01,\n",
            "          1.1812e-01, -8.7310e-02,  1.2170e-01, -4.2251e-01,  7.2562e-01,\n",
            "          1.2957e-01,  3.1246e-01,  7.7235e-01,  8.6660e-01, -1.6929e+00,\n",
            "          3.8453e-01,  2.6660e-01, -3.5932e-01, -5.3297e-01,  2.4182e-01,\n",
            "         -1.6108e-02,  6.9859e-01, -8.3851e-02,  3.2378e-01, -5.8433e-01,\n",
            "          5.4991e-01, -5.0340e-01,  3.2105e-01,  9.3149e-01,  2.3061e-01,\n",
            "         -4.0611e-01, -3.5797e-01, -1.0654e-01, -1.6347e+00, -2.6536e-01],\n",
            "        [-3.1652e-01,  2.2160e-01, -8.6617e-01,  7.9985e-02,  2.3342e-01,\n",
            "          2.6490e-01,  1.3990e-01, -2.4724e-01,  1.2807e+00,  2.0206e-01,\n",
            "          1.2521e-01,  1.3985e-01, -3.1918e-01,  1.9195e-01,  1.0948e-01,\n",
            "          1.1922e+00, -8.0301e-01,  1.4168e-02, -6.7335e-02, -3.6167e-01,\n",
            "         -1.7692e-01, -1.1147e+00, -1.7889e-02,  1.8887e-01,  2.0961e-01,\n",
            "          9.8772e-02, -7.4045e-02,  1.9529e-01, -1.6632e-01, -6.7967e-03,\n",
            "          1.4370e-01, -2.6447e-01, -6.8561e-01,  3.9055e-01,  2.7474e-01,\n",
            "          2.8242e-01,  7.1794e-02,  3.4830e-01,  2.6450e-01, -1.2407e+00,\n",
            "          2.6298e-01, -5.4570e-01,  2.3831e-01,  3.1555e-01, -6.6083e-01,\n",
            "         -3.3629e-01, -1.7410e-01,  2.1133e-01,  2.9142e-01, -9.5955e-02,\n",
            "         -5.8478e-01, -6.3186e-01,  2.4388e-01, -8.5771e-01, -4.9523e-01,\n",
            "          2.0578e-01, -5.3938e-01,  4.4631e-01, -4.4119e-01,  2.3423e-01,\n",
            "         -6.9582e-01,  3.0660e-01, -4.1756e-01, -2.5471e-01, -5.1603e-01,\n",
            "         -3.8536e-01, -3.5051e-01,  8.3775e-02, -6.2377e-01,  2.7930e-02,\n",
            "          4.6581e-02,  1.1321e-01,  1.4397e-01,  3.1622e-01, -1.1032e+00,\n",
            "         -2.2410e-01, -8.7236e-01, -2.0132e-01, -9.4418e-02, -5.3068e-01,\n",
            "         -4.6702e-01, -1.8401e-01,  2.7467e-01, -8.1159e-01,  8.4539e-01,\n",
            "         -1.8689e-02, -7.4075e-01,  2.9425e-02,  2.7118e-01,  2.3844e-02,\n",
            "         -4.1683e-01,  4.6105e-01,  1.2616e-03, -4.8938e-01, -1.9023e-01,\n",
            "         -4.3970e-01,  2.9393e-01,  2.2630e-01, -3.3517e-01,  2.3552e-01],\n",
            "        [ 2.0087e-01, -5.4241e-01,  4.3128e-02, -1.0041e+00, -2.0276e-01,\n",
            "         -2.6807e-01,  2.6491e-01, -2.1257e-01, -3.9234e-01, -1.5054e-01,\n",
            "         -4.9645e-01, -4.0132e-01, -1.5022e-01, -2.6714e-01,  2.6092e-01,\n",
            "         -1.9984e-01, -8.3248e-01,  2.0202e-01,  2.2992e-01, -1.0558e+00,\n",
            "          4.4652e-01, -3.6738e-01, -2.6847e-02, -5.5238e-01, -1.0999e-01,\n",
            "         -1.7881e-01, -7.2223e-02, -3.6510e-01, -2.1272e-01, -7.8545e-01,\n",
            "          1.1215e-01, -3.0620e-02, -4.5098e-01, -2.6592e-01, -1.9802e-01,\n",
            "         -2.5428e-01, -7.1176e-02, -2.1382e-01, -8.0232e-02,  7.7700e-01,\n",
            "         -2.5774e-01,  7.2816e-01,  2.6886e-01, -4.2310e-01, -4.4977e-01,\n",
            "          9.2888e-02,  1.1916e-01, -2.5458e-01, -3.3708e-01,  3.3793e-02,\n",
            "         -1.0974e+00, -6.7795e-01, -2.5988e-01, -4.0098e-01, -1.5917e+00,\n",
            "         -2.0821e-02,  8.8582e-01, -1.4780e+00,  9.6211e-02, -7.2444e-01,\n",
            "          1.2923e-01, -2.0724e-01,  2.9024e-02,  8.9499e-01, -2.0505e-01,\n",
            "          5.3178e-01, -4.4585e-01, -1.5202e-01, -4.9057e-01, -8.0573e-02,\n",
            "          6.0961e-02,  2.5850e-01,  4.4552e-01, -1.9563e-01,  1.3718e-01,\n",
            "          1.0279e-01, -4.9810e-02, -1.2652e+00, -1.6347e-01,  7.2087e-01,\n",
            "          1.4354e+00, -2.1855e-02, -2.6733e-01, -5.3856e-01, -3.3871e-01,\n",
            "         -1.1580e+00, -5.0460e-01,  2.6540e-03,  1.8211e-01,  4.6004e-02,\n",
            "          3.2650e-01, -1.8724e-01, -1.2944e-01, -3.4239e-01, -2.0169e-01,\n",
            "          2.2358e-01, -2.5238e-01, -1.4184e-01, -1.0541e+00, -2.5290e-01]])\n",
            "Name---> output_layer.bias \n",
            "Values---> tensor([-0.0292,  1.6416, -0.9879, -1.3377])\n",
            "-------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           b       0.91      0.93      0.92     23183\n",
            "           t       0.92      0.91      0.92     21663\n",
            "           e       0.94      0.98      0.96     30482\n",
            "           m       0.97      0.84      0.90      9122\n",
            "\n",
            "    accuracy                           0.93     84450\n",
            "   macro avg       0.94      0.91      0.92     84450\n",
            "weighted avg       0.93      0.93      0.93     84450\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# used code from lecture notebook but exchanged the data\n",
        "vocab = Counter()\n",
        "for text in training_data[\"TITLE\"]:\n",
        "    for word in text.split(\" \"):\n",
        "        vocab[word.lower()] += 1\n",
        "\n",
        "for text in testing_data[\"TITLE\"]:\n",
        "    for word in text.split(\" \"):\n",
        "        vocab[word.lower()] += 1\n",
        "\n",
        "total_words = len(vocab)\n",
        "\n",
        "\n",
        "def get_word_2_index(vocab):\n",
        "    word2index = {}\n",
        "    for i, word in enumerate(vocab):\n",
        "        word2index[word.lower()] = i\n",
        "    return word2index\n",
        "\n",
        "\n",
        "word2index = get_word_2_index(vocab)\n",
        "\n",
        "\n",
        "def get_batch(df, i, batch_size):\n",
        "    batches = []\n",
        "    results = []\n",
        "\n",
        "    # used iloc from pandas package because working with dataframe not array\n",
        "    # extracting batch of data from dataframe\n",
        "    texts = df[\"TITLE\"].iloc[i * batch_size : i * batch_size + batch_size]\n",
        "    categories = df[\"CATEGORY\"].iloc[i * batch_size : i * batch_size + batch_size]\n",
        "\n",
        "    for text in texts:\n",
        "        layer = np.zeros(total_words, dtype=float)\n",
        "        for word in text.split(\" \"):\n",
        "            layer[word2index[word.lower()]] += 1\n",
        "        batches.append(layer)\n",
        "\n",
        "    # convert categories to numbers\n",
        "    for category in categories:\n",
        "        index_y = -1\n",
        "        if category == \"b\":\n",
        "            index_y = 0\n",
        "        elif category == \"t\":\n",
        "            index_y = 1\n",
        "        elif category == \"e\":\n",
        "            index_y = 2\n",
        "        elif category == \"m\":\n",
        "            index_y = 3\n",
        "        results.append(index_y)\n",
        "\n",
        "    return np.array(batches), np.array(results)\n",
        "\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.05\n",
        "num_epochs = (\n",
        "    1  # changed epoch size so training is faster, you can increase it if you want\n",
        ")\n",
        "batch_size = 150\n",
        "display_step = 1\n",
        "\n",
        "# Network Parameters\n",
        "hidden_size = 100  # 1st layer and 2nd layer number of feature\n",
        "input_size = total_words  # Words in vocab\n",
        "print(input_size)\n",
        "print(\"--------------------\")\n",
        "num_classes = 4\n",
        "\n",
        "# select gpu (cuda) as method for faster training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Verwendetes Gerät:\", device)\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.empty_cache()  # empty cache -> otherwise there were sometimes errors\n",
        "\n",
        "\n",
        "class NewsNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NewsNN, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, hidden_size, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer_1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer_2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# with \"to()\" you can easily switch between CPU and GPU without changing the rest of your code\n",
        "# had some problems with it so we added it\n",
        "news_net = NewsNN(input_size, hidden_size, num_classes).to(device)\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # This includes the Softmax loss function\n",
        "optimizer = torch.optim.Adam(news_net.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the Model\n",
        "for epoch in range(num_epochs):\n",
        "    # determine the number of min-batches based on the batch size and size of training data - exchanged the data\n",
        "    total_batch = int(len(training_data) / batch_size)\n",
        "    # Loop over all batches\n",
        "    for i in range(total_batch):\n",
        "        batch_x, batch_y = get_batch(training_data, i, batch_size)\n",
        "        articles = torch.FloatTensor(batch_x).to(device)\n",
        "        labels = torch.LongTensor(batch_y).to(device)\n",
        "        # print(\"articles\",articles)\n",
        "        # print(batch_x, labels)\n",
        "        # print(\"size labels\",labels.size())\n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()  # zero the gradient buffer\n",
        "        outputs = news_net(articles)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 4 == 0:\n",
        "            print(\n",
        "                \"Epoch [%d/%d], Step [%d/%d], Loss: %.4f\"\n",
        "                % (\n",
        "                    epoch + 1,\n",
        "                    num_epochs,\n",
        "                    i + 1,\n",
        "                    len(training_data) / batch_size,\n",
        "                    loss.data,\n",
        "                )\n",
        "            )\n",
        "\n",
        "# Save the model\n",
        "torch.save(news_net.state_dict(), 'models/taskthree.pth')\n",
        "\n",
        "# show the different trained parameters\n",
        "for name, param in news_net.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(\"Name--->\", name, \"\\nValues--->\", param.data)\n",
        "\n",
        "# set model to evaluation mode\n",
        "news_net.eval()\n",
        "total_test_batches = int(len(testing_data) / batch_size)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # create empty result arrays\n",
        "    all_predicted = []\n",
        "    all_labels = []\n",
        "    # iterate through each of the batches\n",
        "    for i in range(total_test_batches):\n",
        "        # get data of corresponding batch\n",
        "        test_batch_x, test_batch_y = get_batch(testing_data, i, batch_size)\n",
        "        test_articles = torch.FloatTensor(test_batch_x).to(device)\n",
        "        test_labels = torch.LongTensor(test_batch_y).to(device)\n",
        "        # get data into NN and get predicted labels\n",
        "        test_outputs = news_net(test_articles)\n",
        "        _, predicted = torch.max(test_outputs.data, 1)\n",
        "        \n",
        "        # we need .cpu() because we did .to(device) which was mainly gpu\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(test_labels.cpu().numpy())\n",
        "\n",
        "# print / create classification report for the predicted data\n",
        "print(\"-------------------------------\")\n",
        "print(\n",
        "    classification_report(all_labels, all_predicted, target_names=[\"b\", \"t\", \"e\", \"m\"])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjjpDsBbr-jS"
      },
      "source": [
        "# Task 4\n",
        "\n",
        "Use a pre-trained embeddings and compare your result. When you use pre-trained\n",
        "embeddings, you have to average the word embeddings of each tokens in ach\n",
        "document to get the unique representation of the document. DOC_EMBEDDING =\n",
        "(TOKEN1_EMBEDDING + ... + TOKENn_EMBEDDING). You can also use some of the\n",
        "spacy/FLAIR document embedding methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['e' 't' 'm' 'b']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\JKlass\\Documents\\UNI_HH\\NLP_ML_flask\\venv\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "c:\\Users\\JKlass\\Documents\\UNI_HH\\NLP_ML_flask\\venv\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "NewsNN.__init__() takes 1 positional argument but 4 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\JKlass\\Documents\\UNI_HH\\NLP_ML_flask\\Assignment_machine_learning.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JKlass/Documents/UNI_HH/NLP_ML_flask/Assignment_machine_learning.ipynb#X13sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JKlass/Documents/UNI_HH/NLP_ML_flask/Assignment_machine_learning.ipynb#X13sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39m# create model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JKlass/Documents/UNI_HH/NLP_ML_flask/Assignment_machine_learning.ipynb#X13sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m news_nn \u001b[39m=\u001b[39m NewsNN(input_size, hidden_dim, num_classes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JKlass/Documents/UNI_HH/NLP_ML_flask/Assignment_machine_learning.ipynb#X13sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m# Loss and optimizer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JKlass/Documents/UNI_HH/NLP_ML_flask/Assignment_machine_learning.ipynb#X13sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m loss_function \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n",
            "File \u001b[1;32mc:\\Users\\JKlass\\Documents\\UNI_HH\\NLP_ML_flask\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:464\u001b[0m, in \u001b[0;36mModule.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.__init__() got an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(kwargs))))\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_super_init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mbool\u001b[39m(args):\n\u001b[1;32m--> 464\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.__init__() takes 1 positional argument but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m were\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    465\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    467\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39mCalls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39mto avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39mhandling for parameters, submodules, and buffers but simply calls into\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39msuper().__setattr__ for all other attributes.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[1;31mTypeError\u001b[0m: NewsNN.__init__() takes 1 positional argument but 4 were given"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "import torch.nn as nn\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# read data\n",
        "data = pd.read_csv(\"data/uci-news-aggregator.csv\")\n",
        "# remove unnecessary columns\n",
        "frame = data[[\"TITLE\", \"CATEGORY\"]]\n",
        "# uncomment this cell if you have enough performance\n",
        "frame = frame.head(10000)\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "# Division into training and test data. The stratify parameter causes the \"Category\" feature to be split equally\n",
        "training_data, testing_data = train_test_split(\n",
        "    frame, test_size=TEST_SIZE, random_state=0, stratify=frame[\"CATEGORY\"]\n",
        ")\n",
        "\n",
        "# Set hyperparameters\n",
        "input_size = (\n",
        "    96  # spacy provides vectors with dimension of 96 thats why we need to set that size\n",
        ")\n",
        "num_classes = 4\n",
        "hidden_dim = 100\n",
        "num_epochs = 1\n",
        "batch_size = 150\n",
        "learning_rate = 0.01\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "\n",
        "\n",
        "def average_embedding_token(text):\n",
        "    doc = nlp(text)\n",
        "    # use vector parameter of spacy\n",
        "    embeddings = [token.vector for token in doc]\n",
        "    # if token exists in spacy return value otherwise return zero array\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(input_size)\n",
        "\n",
        "\n",
        "# print unique categories\n",
        "print(training_data[\"CATEGORY\"].unique())\n",
        "\n",
        "# tokenize and apply spacy embeddings to dataset\n",
        "training_data[\"SPACY_EMBEDDING\"] = training_data[\"TITLE\"].apply(average_embedding_token)\n",
        "testing_data[\"SPACY_EMBEDDING\"] = testing_data[\"TITLE\"].apply(average_embedding_token)\n",
        "\n",
        "# dictionary with name -> int class\n",
        "mapping = {\"b\": 0, \"t\": 1, \"e\": 2, \"m\": 3}\n",
        "# labels need to be converted to tensors for training model\n",
        "train_conv_label = torch.tensor(training_data[\"CATEGORY\"].map(mapping).to_numpy())\n",
        "# embeddings need to be converted to tensors for training model\n",
        "train_conv_features = torch.tensor(\n",
        "    np.vstack(training_data[\"SPACY_EMBEDDING\"].to_numpy())\n",
        ")\n",
        "\n",
        "\n",
        "class NewsNN(nn.Module):\n",
        "    def _init_(self, input_size, hidden_size, num_classes):\n",
        "        super(NewsNN, self)._init_()\n",
        "        self.layer_1 = nn.Linear(input_size, hidden_size, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer_1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer_2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# create model\n",
        "news_nn = NewsNN(input_size, hidden_dim, num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(news_nn.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(train_conv_features), batch_size):\n",
        "        data_f = train_conv_features[i : i + batch_size]\n",
        "        labels = train_conv_label[i : i + batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = news_nn(data_f.float())\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if ((i // batch_size) + 1) % 4 == 0:\n",
        "            print(\n",
        "                \"Epoch [%d/%d], Step [%d/%d], Loss: %.4f\"\n",
        "                % (\n",
        "                    epoch + 1,\n",
        "                    num_epochs,\n",
        "                    i // batch_size,\n",
        "                    len(train_conv_features) // batch_size,\n",
        "                    loss.data,\n",
        "                )\n",
        "            )\n",
        "\n",
        "test_conv_features = torch.tensor(np.vstack(testing_data[\"SPACY_EMBEDDING\"].to_numpy()))\n",
        "test_conv_label = torch.tensor(testing_data[\"CATEGORY\"].map(mapping).to_numpy())\n",
        "# Evaluate on the test set\n",
        "with torch.no_grad():\n",
        "    output = news_nn(test_conv_features.float())\n",
        "    _, test_pred = torch.max(output, 1)\n",
        "\n",
        "test_pred = test_pred.numpy()\n",
        "test_conv_label = test_conv_label.numpy()\n",
        "\n",
        "# print classification report with metrics\n",
        "print(\n",
        "    f\"Classification Report: {classification_report(test_conv_label, test_pred, target_names=mapping, zero_division=1)}\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
