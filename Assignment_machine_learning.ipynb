{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6x3Hn9MIb9k"
      },
      "source": [
        "# Assignment <span style=\"color:red\">option Four</span> - News Categorization using PyTorch\n",
        "\n",
        "Download the dataset from https://www.kaggle.com/uciml/news-aggregator-dataset and develop a news classification or categorization model. The dataset contain only titles of a news item and some metadata. The categories of the news items include one of: –<span  style=\"color:red\"> b</span> : business – <span  style=\"color:red\">t</span> : science and technology – <span  style=\"color:red\">e</span> : entertainment and –<span  style=\"color:red\">m</span> : health.\n",
        "\n",
        "1. Prepare training and test dataset: Split the data into training and test set (80% train and 20% test). Make sure they are balanced, otherwise if all <span  style=\"color:red\">b</span> files are on training, your model fails to predict <span  style=\"color:red\">t</span> files in test.\n",
        "2. Binary classification: produce training data for each two categories, such as <span  style=\"color:red\">b </span> and <span  style=\"color:red\"> t</span>, <span  style=\"color:red\">b</span> and <span  style=\"color:red\"> m</span>, <span  style=\"color:red\">e</span> and <span  style=\"color:red\">t</span> and so on. Evaluate the performance and report which categories are easier for the models.\n",
        "3. Adapt the Text Categorization PyTorch code (see above) and evaluate the performance of the system for these task\n",
        "4. Use a pre-trained embeddings and compare your result. When you use pre-trained embeddings, you have to average the word embeddings of each tokens in ach document to get the unique representation of the document. DOC_EMBEDDING = (TOKEN1_EMBEDDING + ... + TOKENn_EMBEDDING). You can also use some of the <span  style=\"color:red\">spacy/FLAIR </span>document embedding methods\n",
        "5. Report the recall, precision, and F1 scores for both binary and multi-class classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwyNWM5nr-jL"
      },
      "source": [
        "# Task 1\n",
        "\n",
        "1. Prepare training and test dataset: Split the data into training and test set (80% train and 20% test). Make sure they are balanced, otherwise if all <span  style=\"color:red\">b</span> files are on training, your model fails to predict <span  style=\"color:red\">t</span> files in test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RnOhGjir-jM",
        "outputId": "c7a099b9-39de-4432-9494-f6f43df4d83b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainingsdaten:  337935\n",
            "Testdaten:  84484\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# read data\n",
        "data = pd.read_csv(\"data/uci-news-aggregator.csv\")\n",
        "# remove unnecessary columns\n",
        "frame = data[[\"TITLE\", \"CATEGORY\"]]\n",
        "\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "# Division into training and test data. The stratify parameter causes the \"Category\" feature to be split equally\n",
        "training_data, testing_data = train_test_split(\n",
        "    frame, test_size=TEST_SIZE, random_state=0, stratify=data[\"CATEGORY\"]\n",
        ")\n",
        "\n",
        "# print size of train and test set\n",
        "print(\"Trainingsdaten: \", len(training_data))\n",
        "print(\"Testdaten: \", len(testing_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLqPPvzgr-jO"
      },
      "source": [
        "# Task 2\n",
        "\n",
        "Binary classification: produce training data for each two categories, such as b and t, b\n",
        "and m, e and t and so on. Evaluate the performance and report which categories are\n",
        "easier for the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WI4nJ3Wr-jP",
        "outputId": "2046c38a-bb94-4589-f504-5c024f375b7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_32372\\1080574781.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_32372\\1080574781.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Category Pair: b (1) vs t (0)\n",
            "------------------PERFORMANCE-----------------------------\n",
            "Accuracy: 0.93\n",
            "Precision: 0.93\n",
            "Recall: 0.93\n",
            "F1_score: 0.93\n",
            "--------------------REPORT--------------------------------\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.92     21669\n",
            "           1       0.93      0.93      0.93     23193\n",
            "\n",
            "    accuracy                           0.93     44862\n",
            "   macro avg       0.93      0.93      0.93     44862\n",
            "weighted avg       0.93      0.93      0.93     44862\n",
            "\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_32372\\1080574781.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_32372\\1080574781.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Category Pair: b (1) vs e (0)\n",
            "------------------PERFORMANCE-----------------------------\n",
            "Accuracy: 0.98\n",
            "Precision: 0.98\n",
            "Recall: 0.97\n",
            "F1_score: 0.97\n",
            "--------------------REPORT--------------------------------\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98     30494\n",
            "           1       0.98      0.97      0.97     23193\n",
            "\n",
            "    accuracy                           0.98     53687\n",
            "   macro avg       0.98      0.98      0.98     53687\n",
            "weighted avg       0.98      0.98      0.98     53687\n",
            "\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_32372\\1080574781.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_32372\\1080574781.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Category Pair: b (1) vs m (0)\n",
            "------------------PERFORMANCE-----------------------------\n",
            "Accuracy: 0.97\n",
            "Precision: 0.97\n",
            "Recall: 0.99\n",
            "F1_score: 0.98\n",
            "--------------------REPORT--------------------------------\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95      9128\n",
            "           1       0.97      0.99      0.98     23193\n",
            "\n",
            "    accuracy                           0.97     32321\n",
            "   macro avg       0.97      0.96      0.97     32321\n",
            "weighted avg       0.97      0.97      0.97     32321\n",
            "\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_32372\\1080574781.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_32372\\1080574781.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Category Pair: t (1) vs e (0)\n",
            "------------------PERFORMANCE-----------------------------\n",
            "Accuracy: 0.98\n",
            "Precision: 0.97\n",
            "Recall: 0.97\n",
            "F1_score: 0.97\n",
            "--------------------REPORT--------------------------------\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98     30494\n",
            "           1       0.97      0.97      0.97     21669\n",
            "\n",
            "    accuracy                           0.98     52163\n",
            "   macro avg       0.98      0.98      0.98     52163\n",
            "weighted avg       0.98      0.98      0.98     52163\n",
            "\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_32372\\1080574781.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_32372\\1080574781.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Category Pair: t (1) vs m (0)\n",
            "------------------PERFORMANCE-----------------------------\n",
            "Accuracy: 0.98\n",
            "Precision: 0.97\n",
            "Recall: 0.99\n",
            "F1_score: 0.98\n",
            "--------------------REPORT--------------------------------\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96      9128\n",
            "           1       0.97      0.99      0.98     21669\n",
            "\n",
            "    accuracy                           0.98     30797\n",
            "   macro avg       0.98      0.97      0.97     30797\n",
            "weighted avg       0.98      0.98      0.98     30797\n",
            "\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_32372\\1080574781.py:42: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
            "C:\\Users\\JKlass\\AppData\\Local\\Temp\\ipykernel_32372\\1080574781.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "Category Pair: e (1) vs m (0)\n",
            "------------------PERFORMANCE-----------------------------\n",
            "Accuracy: 0.98\n",
            "Precision: 0.98\n",
            "Recall: 0.99\n",
            "F1_score: 0.99\n",
            "--------------------REPORT--------------------------------\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.95      9128\n",
            "           1       0.98      0.99      0.99     30494\n",
            "\n",
            "    accuracy                           0.98     39622\n",
            "   macro avg       0.98      0.96      0.97     39622\n",
            "weighted avg       0.98      0.98      0.98     39622\n",
            "\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Define the categories\n",
        "categories = [\"b\", \"t\", \"e\", \"m\"]\n",
        "\n",
        "# get all possible combinations\n",
        "combinations_categories = list(combinations(categories, 2))\n",
        "\n",
        "# print combinations\n",
        "# for combination in possible_combinations:\n",
        "#    print(combination)\n",
        "\n",
        "# loop through each category combination\n",
        "for category_pair in combinations_categories:\n",
        "    category_1, category_2 = category_pair\n",
        "\n",
        "    # only keep data of category pair\n",
        "    filtered_training_data = training_data[\n",
        "        (training_data[\"CATEGORY\"] == category_1)\n",
        "        | (training_data[\"CATEGORY\"] == category_2)\n",
        "    ]\n",
        "    filtered_test_data = testing_data[\n",
        "        (testing_data[\"CATEGORY\"] == category_1)\n",
        "        | (testing_data[\"CATEGORY\"] == category_2)\n",
        "    ]\n",
        "\n",
        "    # Create a binary dataset for the current category pair\n",
        "    cat_mapping = {category_1: 1, category_2: 0}\n",
        "    filtered_training_data[\"CATEGORY_IN_BINARY\"] = filtered_training_data[\n",
        "        \"CATEGORY\"\n",
        "    ].map(cat_mapping)\n",
        "    filtered_test_data[\"CATEGORY_IN_BINARY\"] = filtered_test_data[\"CATEGORY\"].map(\n",
        "        cat_mapping\n",
        "    )\n",
        "\n",
        "    # print(filtered_training_data)\n",
        "\n",
        "    # split the binary dataset into features (X) und labels (y)\n",
        "    X_train = filtered_training_data[\"TITLE\"]\n",
        "    y_train = filtered_training_data[\"CATEGORY_IN_BINARY\"]\n",
        "    X_test = filtered_test_data[\"TITLE\"]\n",
        "    y_test = filtered_test_data[\"CATEGORY_IN_BINARY\"]\n",
        "\n",
        "    # vectorize the titles using TF-IDF\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "    # save vectorizer\n",
        "    with open(f'vectorizer/tasktwo_{category_1}_{category_2}.pkl', 'wb') as vectorizer_file:\n",
        "        pickle.dump(vectorizer, vectorizer_file)\n",
        "\n",
        "    # train a Naive Bayes classifier\n",
        "    classifier = MultinomialNB()\n",
        "    classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "\n",
        "\n",
        "    # Save the trained model\n",
        "    with open(f'models/tasktwo_{category_1}_{category_2}.pkl', 'wb') as model_file:\n",
        "        pickle.dump(classifier, model_file)\n",
        "\n",
        "    # make predictions on the test set\n",
        "    predictions = classifier.predict(X_test_tfidf)\n",
        "\n",
        "    # evaluate performance\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions)\n",
        "    recall = recall_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions)\n",
        "\n",
        "    # this report gives further information\n",
        "    report = classification_report(y_test, predictions)\n",
        "\n",
        "    # print results\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    print(\n",
        "        f\"Category Pair: {category_1} ({cat_mapping[category_1]}) vs {category_2} ({cat_mapping[category_2]})\"\n",
        "    )\n",
        "    print(\"------------------PERFORMANCE-----------------------------\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1_score: {f1:.2f}\")\n",
        "    print(\"--------------------REPORT--------------------------------\")\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT_Ao7jPr-jQ"
      },
      "source": [
        "# Task 3\n",
        "\n",
        "Adapt the Text Categorization PyTorch code (see above) and evaluate the performance\n",
        "of the system for these task\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg_zbCysYO_B",
        "outputId": "ed5c1a4b-f6b2-4034-c28d-79fbef226d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "135402\n",
            "--------------------\n",
            "Verwendetes Gerät: cpu\n",
            "Epoch [1/1], Step [4/2252], Loss: 1.1010\n",
            "Epoch [1/1], Step [8/2252], Loss: 0.8581\n",
            "Epoch [1/1], Step [12/2252], Loss: 0.7082\n",
            "Epoch [1/1], Step [16/2252], Loss: 0.8497\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\JKlass\\Documents\\UNI_HH\\NLP_ML_flask\\Assignment_machine_learning.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/JKlass/Documents/UNI_HH/NLP_ML_flask/Assignment_machine_learning.ipynb#X10sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39m# Loop over all batches\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/JKlass/Documents/UNI_HH/NLP_ML_flask/Assignment_machine_learning.ipynb#X10sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(total_batch):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/JKlass/Documents/UNI_HH/NLP_ML_flask/Assignment_machine_learning.ipynb#X10sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     batch_x, batch_y \u001b[39m=\u001b[39m get_batch(training_data, i, batch_size)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/JKlass/Documents/UNI_HH/NLP_ML_flask/Assignment_machine_learning.ipynb#X10sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m     articles \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(batch_x)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/JKlass/Documents/UNI_HH/NLP_ML_flask/Assignment_machine_learning.ipynb#X10sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(batch_y)\u001b[39m.\u001b[39mto(device)\n",
            "\u001b[1;32mc:\\Users\\JKlass\\Documents\\UNI_HH\\NLP_ML_flask\\Assignment_machine_learning.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JKlass/Documents/UNI_HH/NLP_ML_flask/Assignment_machine_learning.ipynb#X10sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         index_y \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JKlass/Documents/UNI_HH/NLP_ML_flask/Assignment_machine_learning.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     results\u001b[39m.\u001b[39mappend(index_y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JKlass/Documents/UNI_HH/NLP_ML_flask/Assignment_machine_learning.ipynb#X10sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49marray(batches), np\u001b[39m.\u001b[39marray(results)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# used code from lecture notebook but exchanged the data\n",
        "vocab = Counter()\n",
        "for text in training_data[\"TITLE\"]:\n",
        "    for word in text.split(\" \"):\n",
        "        vocab[word.lower()] += 1\n",
        "\n",
        "for text in testing_data[\"TITLE\"]:\n",
        "    for word in text.split(\" \"):\n",
        "        vocab[word.lower()] += 1\n",
        "\n",
        "total_words = len(vocab)\n",
        "\n",
        "\n",
        "def get_word_2_index(vocab):\n",
        "    word2index = {}\n",
        "    for i, word in enumerate(vocab):\n",
        "        word2index[word.lower()] = i\n",
        "    return word2index\n",
        "\n",
        "\n",
        "word2index = get_word_2_index(vocab)\n",
        "\n",
        "\n",
        "def get_batch(df, i, batch_size):\n",
        "    batches = []\n",
        "    results = []\n",
        "\n",
        "    # used iloc from pandas package because working with dataframe not array\n",
        "    # extracting batch of data from dataframe\n",
        "    texts = df[\"TITLE\"].iloc[i * batch_size : i * batch_size + batch_size]\n",
        "    categories = df[\"CATEGORY\"].iloc[i * batch_size : i * batch_size + batch_size]\n",
        "\n",
        "    for text in texts:\n",
        "        layer = np.zeros(total_words, dtype=float)\n",
        "        for word in text.split(\" \"):\n",
        "            layer[word2index[word.lower()]] += 1\n",
        "        batches.append(layer)\n",
        "\n",
        "    # convert categories to numbers\n",
        "    for category in categories:\n",
        "        index_y = -1\n",
        "        if category == \"b\":\n",
        "            index_y = 0\n",
        "        elif category == \"t\":\n",
        "            index_y = 1\n",
        "        elif category == \"e\":\n",
        "            index_y = 2\n",
        "        elif category == \"m\":\n",
        "            index_y = 3\n",
        "        results.append(index_y)\n",
        "\n",
        "    return np.array(batches), np.array(results)\n",
        "\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.05\n",
        "num_epochs = (\n",
        "    1  # changed epoch size so training is faster, you can increase it if you want\n",
        ")\n",
        "batch_size = 150\n",
        "display_step = 1\n",
        "\n",
        "# Network Parameters\n",
        "hidden_size = 100  # 1st layer and 2nd layer number of feature\n",
        "input_size = total_words  # Words in vocab\n",
        "print(input_size)\n",
        "print(\"--------------------\")\n",
        "num_classes = 4\n",
        "\n",
        "# select gpu (cuda) as method for faster training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Verwendetes Gerät:\", device)\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.empty_cache()  # empty cache -> otherwise there were sometimes errors\n",
        "\n",
        "\n",
        "class NewsNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NewsNN, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, hidden_size, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer_1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer_2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# with \"to()\" you can easily switch between CPU and GPU without changing the rest of your code\n",
        "# had some problems with it so we added it\n",
        "news_net = NewsNN(input_size, hidden_size, num_classes).to(device)\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # This includes the Softmax loss function\n",
        "optimizer = torch.optim.Adam(news_net.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the Model\n",
        "for epoch in range(num_epochs):\n",
        "    # determine the number of min-batches based on the batch size and size of training data - exchanged the data\n",
        "    total_batch = int(len(training_data) / batch_size)\n",
        "    # Loop over all batches\n",
        "    for i in range(total_batch):\n",
        "        batch_x, batch_y = get_batch(training_data, i, batch_size)\n",
        "        articles = torch.FloatTensor(batch_x).to(device)\n",
        "        labels = torch.LongTensor(batch_y).to(device)\n",
        "        # print(\"articles\",articles)\n",
        "        # print(batch_x, labels)\n",
        "        # print(\"size labels\",labels.size())\n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()  # zero the gradient buffer\n",
        "        outputs = news_net(articles)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 4 == 0:\n",
        "            print(\n",
        "                \"Epoch [%d/%d], Step [%d/%d], Loss: %.4f\"\n",
        "                % (\n",
        "                    epoch + 1,\n",
        "                    num_epochs,\n",
        "                    i + 1,\n",
        "                    len(training_data) / batch_size,\n",
        "                    loss.data,\n",
        "                )\n",
        "            )\n",
        "\n",
        "# Save the model\n",
        "torch.save(news_net.state_dict(), 'models/taskthree.pth')\n",
        "\n",
        "# show the different trained parameters\n",
        "for name, param in news_net.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(\"Name--->\", name, \"\\nValues--->\", param.data)\n",
        "\n",
        "# set model to evaluation mode\n",
        "news_net.eval()\n",
        "total_test_batches = int(len(testing_data) / batch_size)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # create empty result arrays\n",
        "    all_predicted = []\n",
        "    all_labels = []\n",
        "    # iterate through each of the batches\n",
        "    for i in range(total_test_batches):\n",
        "        # get data of corresponding batch\n",
        "        test_batch_x, test_batch_y = get_batch(testing_data, i, batch_size)\n",
        "        test_articles = torch.FloatTensor(test_batch_x).to(device)\n",
        "        test_labels = torch.LongTensor(test_batch_y).to(device)\n",
        "        # get data into NN and get predicted labels\n",
        "        test_outputs = news_net(test_articles)\n",
        "        _, predicted = torch.max(test_outputs.data, 1)\n",
        "        \n",
        "        # we need .cpu() because we did .to(device) which was mainly gpu\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(test_labels.cpu().numpy())\n",
        "\n",
        "# print / create classification report for the predicted data\n",
        "print(\"-------------------------------\")\n",
        "print(\n",
        "    classification_report(all_labels, all_predicted, target_names=[\"b\", \"t\", \"e\", \"m\"])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjjpDsBbr-jS"
      },
      "source": [
        "# Task 4\n",
        "\n",
        "Use a pre-trained embeddings and compare your result. When you use pre-trained\n",
        "embeddings, you have to average the word embeddings of each tokens in ach\n",
        "document to get the unique representation of the document. DOC_EMBEDDING =\n",
        "(TOKEN1_EMBEDDING + ... + TOKENn_EMBEDDING). You can also use some of the\n",
        "spacy/FLAIR document embedding methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1SX6SR5j-kj"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Network Parameters\n",
        "hidden_size = 100  # 1st layer and 2nd layer number of feature\n",
        "input_size = total_words  # Words in vocab\n",
        "\n",
        "# Set hyperparameters\n",
        "embedding_dim = 300  # SpaCy provides 300-dimensional word vectors\n",
        "num_classes = 4\n",
        "hidden_dim = 100\n",
        "num_epochs = 1\n",
        "batch_size = 150\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Tokenization and embeddings using spacy with the larger English model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "\n",
        "\n",
        "def calculate_average_embedding(text):\n",
        "    doc = nlp(text)\n",
        "    # Use the vector attribute to get the word vectors\n",
        "    embeddings = [token.vector for token in doc]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(embedding_dim)\n",
        "\n",
        "\n",
        "# Apply tokenization and embeddings to the dataset\n",
        "training_data[\"SPACY_EMBEDDING\"] = training_data[\"TITLE\"].apply(\n",
        "    calculate_average_embedding\n",
        ")\n",
        "testing_data[\"SPACY_EMBEDDING\"] = testing_data[\"TITLE\"].apply(\n",
        "    calculate_average_embedding\n",
        ")\n",
        "\n",
        "# Convert embeddings to torch tensors\n",
        "train_embeddings = torch.tensor(np.vstack(training_data[\"SPACY_EMBEDDING\"].to_numpy()))\n",
        "test_embeddings = torch.tensor(np.vstack(testing_data[\"SPACY_EMBEDDING\"].to_numpy()))\n",
        "\n",
        "label_mapping = {\"b\": 0, \"t\": 1, \"e\": 2, \"m\": 3}\n",
        "\n",
        "# Convert labels to torch tensors\n",
        "train_labels = torch.tensor(training_data[\"CATEGORY\"].map(label_mapping).to_numpy())\n",
        "test_labels = torch.tensor(testing_data[\"CATEGORY\"].map(label_mapping).to_numpy())\n",
        "\n",
        "# Instantiate the model\n",
        "model = NewsNN(embedding_dim, hidden_dim, num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(train_embeddings), batch_size):\n",
        "        inputs = train_embeddings[i : i + batch_size]\n",
        "        labels = train_labels[i : i + batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if ((i // batch_size) + 1) % 2 == 0:\n",
        "            print(\n",
        "                \"Epoch [%d/%d], Step [%d/%d], Loss: %.4f\"\n",
        "                % (\n",
        "                    epoch + 1,\n",
        "                    num_epochs,\n",
        "                    i // batch_size,\n",
        "                    len(train_embeddings) // batch_size,\n",
        "                    loss.data,\n",
        "                )\n",
        "            )\n",
        "\n",
        "# Evaluate on the test set\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_embeddings.float())\n",
        "    _, test_predictions = torch.max(test_outputs, 1)\n",
        "\n",
        "test_predictions = test_predictions.numpy()\n",
        "test_labels = test_labels.numpy()\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels, test_predictions, target_names=label_mapping))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HpuGterpBQs",
        "outputId": "e3a50d8d-fc1b-4e19-b502-34b4d1275859"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:46, 5.19MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [01:02<00:00, 6414.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1], Step [1/112], Loss: 1.2954\n",
            "Epoch [1/1], Step [3/112], Loss: 1.1915\n",
            "Epoch [1/1], Step [5/112], Loss: 1.1502\n",
            "Epoch [1/1], Step [7/112], Loss: 1.0810\n",
            "Epoch [1/1], Step [9/112], Loss: 1.0278\n",
            "Epoch [1/1], Step [11/112], Loss: 1.0270\n",
            "Epoch [1/1], Step [13/112], Loss: 0.9577\n",
            "Epoch [1/1], Step [15/112], Loss: 0.9733\n",
            "Epoch [1/1], Step [17/112], Loss: 0.9294\n",
            "Epoch [1/1], Step [19/112], Loss: 0.9462\n",
            "Epoch [1/1], Step [21/112], Loss: 0.9344\n",
            "Epoch [1/1], Step [23/112], Loss: 0.9276\n",
            "Epoch [1/1], Step [25/112], Loss: 0.9029\n",
            "Epoch [1/1], Step [27/112], Loss: 0.9286\n",
            "Epoch [1/1], Step [29/112], Loss: 0.8933\n",
            "Epoch [1/1], Step [31/112], Loss: 0.8681\n",
            "Epoch [1/1], Step [33/112], Loss: 0.8798\n",
            "Epoch [1/1], Step [35/112], Loss: 0.8652\n",
            "Epoch [1/1], Step [37/112], Loss: 0.8738\n",
            "Epoch [1/1], Step [39/112], Loss: 0.8865\n",
            "Epoch [1/1], Step [41/112], Loss: 0.8775\n",
            "Epoch [1/1], Step [43/112], Loss: 0.8639\n",
            "Epoch [1/1], Step [45/112], Loss: 0.8455\n",
            "Epoch [1/1], Step [47/112], Loss: 0.8664\n",
            "Epoch [1/1], Step [49/112], Loss: 0.8579\n",
            "Epoch [1/1], Step [51/112], Loss: 0.8609\n",
            "Epoch [1/1], Step [53/112], Loss: 0.8387\n",
            "Epoch [1/1], Step [55/112], Loss: 0.8346\n",
            "Epoch [1/1], Step [57/112], Loss: 0.8604\n",
            "Epoch [1/1], Step [59/112], Loss: 0.8467\n",
            "Epoch [1/1], Step [61/112], Loss: 0.8499\n",
            "Epoch [1/1], Step [63/112], Loss: 0.8277\n",
            "Epoch [1/1], Step [65/112], Loss: 0.8458\n",
            "Epoch [1/1], Step [67/112], Loss: 0.8411\n",
            "Epoch [1/1], Step [69/112], Loss: 0.8300\n",
            "Epoch [1/1], Step [71/112], Loss: 0.8782\n",
            "Epoch [1/1], Step [73/112], Loss: 0.8321\n",
            "Epoch [1/1], Step [75/112], Loss: 0.8658\n",
            "Epoch [1/1], Step [77/112], Loss: 0.8340\n",
            "Epoch [1/1], Step [79/112], Loss: 0.8311\n",
            "Epoch [1/1], Step [81/112], Loss: 0.8572\n",
            "Epoch [1/1], Step [83/112], Loss: 0.8077\n",
            "Epoch [1/1], Step [85/112], Loss: 0.8148\n",
            "Epoch [1/1], Step [87/112], Loss: 0.8283\n",
            "Epoch [1/1], Step [89/112], Loss: 0.8068\n",
            "Epoch [1/1], Step [91/112], Loss: 0.8340\n",
            "Epoch [1/1], Step [93/112], Loss: 0.8367\n",
            "Epoch [1/1], Step [95/112], Loss: 0.8104\n",
            "Epoch [1/1], Step [97/112], Loss: 0.8239\n",
            "Epoch [1/1], Step [99/112], Loss: 0.8249\n",
            "Epoch [1/1], Step [101/112], Loss: 0.8423\n",
            "Epoch [1/1], Step [103/112], Loss: 0.8405\n",
            "Epoch [1/1], Step [105/112], Loss: 0.8426\n",
            "Epoch [1/1], Step [107/112], Loss: 0.8290\n",
            "Epoch [1/1], Step [109/112], Loss: 0.8072\n",
            "Epoch [1/1], Step [111/112], Loss: 0.8393\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           b       0.59      0.71      0.64     23193\n",
            "           t       0.72      0.43      0.54     21669\n",
            "           e       0.66      0.83      0.73     30494\n",
            "           m       0.81      0.48      0.60      9128\n",
            "\n",
            "    accuracy                           0.65     84484\n",
            "   macro avg       0.69      0.61      0.63     84484\n",
            "weighted avg       0.67      0.65      0.64     84484\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torchtext.vocab import GloVe\n",
        "\n",
        "input_size = 300  # Assuming 300-dimensional GloVe embeddings\n",
        "output_size = 4\n",
        "hidden_size = 100  # 1st layer and 2nd layer number of features\n",
        "num_epochs = 1\n",
        "batch_size = 3000\n",
        "learning_rate = 0.02\n",
        "\n",
        "label_mapping = {\"b\": 0, \"t\": 1, \"e\": 2, \"m\": 3}\n",
        "\n",
        "# Load GloVe embeddings\n",
        "glove = GloVe(name=\"6B\", dim=300)\n",
        "\n",
        "# Tokenization and embeddings using spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "\n",
        "\n",
        "def get_average_embedding(text):\n",
        "    tokens = nlp(text)\n",
        "    embeddings = [\n",
        "        glove[token.text].numpy() for token in tokens if token.text in glove.stoi\n",
        "    ]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(input_size)  # Return zeros if no embeddings are found\n",
        "\n",
        "\n",
        "# Apply tokenization and embeddings to the dataset\n",
        "training_data[\"EMBEDDING_GLOVE\"] = training_data[\"TITLE\"].apply(get_average_embedding)\n",
        "testing_data[\"EMBEDDING_GLOVE\"] = testing_data[\"TITLE\"].apply(get_average_embedding)\n",
        "\n",
        "# Convert embeddings to torch tensors\n",
        "train_embeddings = torch.tensor(np.vstack(training_data[\"EMBEDDING_GLOVE\"].to_numpy()))\n",
        "test_embeddings = torch.tensor(np.vstack(testing_data[\"EMBEDDING_GLOVE\"].to_numpy()))\n",
        "\n",
        "# Convert labels to torch tensors\n",
        "train_labels = torch.tensor(training_data[\"CATEGORY\"].map(label_mapping).to_numpy())\n",
        "test_labels = torch.tensor(testing_data[\"CATEGORY\"].map(label_mapping).to_numpy())\n",
        "\n",
        "# Instantiate the model\n",
        "model = NewsNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(train_embeddings), batch_size):\n",
        "        inputs = train_embeddings[i : i + batch_size]\n",
        "        labels = train_labels[i : i + batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if ((i / batch_size) + 1) % 2 == 0:\n",
        "            print(\n",
        "                \"Epoch [%d/%d], Step [%d/%d], Loss: %.4f\"\n",
        "                % (\n",
        "                    epoch + 1,\n",
        "                    num_epochs,\n",
        "                    i / batch_size,\n",
        "                    len(train_embeddings) / batch_size,\n",
        "                    loss.data,\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "# Evaluate on the test set\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_embeddings.float())\n",
        "    _, test_predictions = torch.max(test_outputs, 1)\n",
        "\n",
        "test_predictions = test_predictions.numpy()\n",
        "test_labels = test_labels.numpy()\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels, test_predictions, target_names=label_mapping))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
